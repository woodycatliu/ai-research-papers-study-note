
---

### 摘要 (Abstract)

**Reflexion** 是一種新穎的框架，旨在透過**語言回饋**來強化大型語言模型（LLM）作為目標導向代理在外部環境（如遊戲、編譯器、API）中的學習能力。傳統的強化學習方法需要大量訓練樣本和昂貴的模型微調，使得 LLM 難以快速有效地從試錯中學習。

Reflexion 的核心概念是讓代理**口頭反思任務回饋訊號**。這些反思文本會儲存在**情景記憶緩衝區**中，以引導代理在後續的試驗中做出更好的決策。此框架具備高度靈活性，可以整合各種**類型**（純量值或自由形式語言）和**來源**（外部或內部模擬）的回饋訊號。

Reflexion 在多樣化任務上，包括循序決策、程式編碼和語言推理，都取得了顯著優於基線代理的成果。例如：

- 在 **HumanEval 編碼基準測試**中，Reflexion 達到了 **91% 的 pass@1 準確度**，超越了先前最先進的 GPT-4 (80%)。
- 研究也包含了消融（ablation）和分析研究，探討不同回饋訊號、回饋整合方法和代理類型對效能的影響。 所有程式碼、示範和資料集都已公開。

### 1 引言 (Introduction)

近期的研究已證明基於大型語言模型（LLM）的自主決策代理是可行的。然而，這些依賴龐大模型的代理在學習方面，目前多數僅限於使用**上下文範例**作為教學方式。這是因為傳統的強化學習優化方案（如梯度下降）需要大量的運算資源和時間。

為了解決這一挑戰，論文提出了 **Reflexion**，這是一種利用**口頭強化**來幫助代理從過去的失敗中學習的方法。其運作原理是將環境提供的**二元或純量回饋**，轉換為**文本摘要形式的口頭回饋**，並作為額外上下文加入 LLM 代理的下一個情景中。這種**自我反思式回饋**如同一個「語義」梯度訊號，為代理提供了具體的改進方向，使其能從錯誤中學習並更好地執行任務。這與人類透過反思過往失敗來形成改進計畫，進而在少數幾次嘗試中迭代學習複雜任務的方式相似。

生成有用的反思性回饋是一項挑戰，因為這需要 LLM 能夠理解錯誤原因（歸因問題），並生成包含可操作改進見解的摘要。研究探索了三種生成方式：

- **簡單的二元環境回饋**。
- 針對常見失敗情況的**預定義啟發式演算法**。
- 使用 LLM 進行的**自我評估**，例如二元分類（決策制定）或自我編寫單元測試（程式編程）。 在所有實施中，評估訊號都會被放大成**自然語言體驗摘要**，可儲存在**長期記憶**中。

Reflexion 相較於傳統強化學習方法，具有以下優勢：

- **輕量化**且**不需微調 LLM**。
- 允許**更細緻的回饋形式**（如針對動作的具體修改），優於難以精確歸因的純量或向量獎勵。
- 提供了**更明確且可解釋的情景記憶**形式。
- 能為未來的情景中的動作提供**更明確的提示**。 然而，其缺點在於**依賴 LLM 的自我評估能力或啟發式演算法**，且**沒有成功的正式保證**。但隨著 LLM 能力的提升，預計此範式將持續改進。

實驗涵蓋了三種類型的任務：

- **決策制定任務**：測試長軌跡中的循序動作選擇。
- **推理任務**：測試知識密集、單步生成的改進。
- **程式編程任務**：教導代理有效使用外部工具，如編譯器和解釋器。 在這些任務中，Reflexion 代理展現出更優異的決策、推理和程式編程能力。具體來說：
- 在 **AlfWorld 決策制定任務**上，比強大基線方法**絕對提高了 22%** (在 12 個迭代學習步驟中)。
- 在 **HotPotQA 推理問題**上**提高了 20%**。
- 在 **HumanEval Python 程式編程任務**上**提高了高達 11%**。

總結而言，研究的主要貢獻包括：

- 提出 **Reflexion**，一種透過將策略參數化為代理的記憶編碼與 LLM 參數選擇相結合的「**口頭強化**」新範式。
- 實證證明 LLM **自我反思的湧現特性**對在少量試驗中學習複雜任務極為有用。
- 引入 **LeetcodeHardGym**，一個包含 40 個困難級 Leetcode 問題（涵蓋 19 種程式語言）的程式碼生成強化學習環境。
- 展示 Reflexion 在多項任務上優於強大基線，並在各種程式碼生成基準測試上取得了**最先進的結果**。

### 2 相關工作 (Related Work)

本節將Reflexion與其他相關研究進行比較，主要分為推理與決策制定以及程式編程兩大領域。

**推理與決策制定**：

- **Self-Refine**：採用迭代框架透過自我評估來自主改進生成，但**僅限於單次生成推理任務**。
- **Pryzant et al.**：執行類似的語義提示編寫優化，但同樣**僅限於單次生成任務**。
- **Paul et al.**：微調批評者模型在軌跡內提供**中間回饋**。
- **Xie et al.**：使用**隨機集束搜尋**實現更高效的決策制定搜尋策略，利用自我評估組件帶來的先見之明。
- **Yoran et al. 和 Nair et al.**：使用決策者模型**推理多個生成**。
- **Kim et al.**：使用**固定步數的重試模式**，但**沒有評估步驟**。
- **Goodman**：執行**定性評估步驟**，提出對先前生成的優化建議。
- **Reflexion**：本論文證明，Reflexion 可以透過**自我反思**來增強上述概念，建立一個**持續存在的自我反思體驗記憶**，使代理能夠識別自身錯誤並自我建議從錯誤中學習的教訓。Reflexion 在決策制定、隱藏約束、決策制定、二元獎勵和記憶方面都展現了優勢。

**程式編程**：

- **AlphaCode**：在隱藏測試案例上評估一組生成內容。
- **CodeT**：使用**自我生成的單元測試**來評分生成的函數實現。
- **Self-Debugging**：使用**除錯組件**，根據程式碼執行環境的回饋改進現有實現。
- **CodeRL**：使用 actor-critic 設定將問題置於強化學習框架中，根據執行環境的回饋除錯程式。
- **AlphaCode、Self-Debugging 和 CodeRL**：雖然能有效修復簡單程式錯誤，但它們**依賴基本事實測試案例**，這會使 pass@1 資格失效，且**不使用自我反思**來連結錯誤識別和實現改進。
- **CodeT**：不存取隱藏測試案例，但**未實施自我學習步驟**以改進程式碼編寫。
- **Reflexion**：本論文提出的方法則**具備測試、除錯、自我生成測試、多語言支援和自我反思**等多項優勢，能有效彌補現有方法的不足。

### 3 Reflexion：透過口頭反思進行強化 (Reflexion: reinforcement via verbal reflection)

Reflexion 框架採用**模組化設計**，由三個主要模型協同運作，並輔以記憶組件：

- **執行者 (Actor)**：記作 (M_a)，負責根據環境觀測生成必要的文字和動作。這類似於傳統強化學習中的策略，可探索多種 Actor 模型，如 **Chain of Thought** 和 **ReAct**。為了提供更多上下文，還加入了**記憶組件 `mem`**，其靈感來自於「上下文學習」方法。
- **評估者 (Evaluator)**：記作 (M_e)，關鍵作用是評估 Actor 產出的品質，確保其符合任務要求。由於定義語義空間的獎勵函數困難，Evaluator 探索了多種變體：
    - **推理任務**：基於**精確匹配 (EM)** 進行評分。
    - **決策制定任務**：使用**預定義的啟發式函數**。
    - **通用方法**：直接使用另一個 **LLM** 作為 Evaluator 來生成決策和程式編程任務的獎勵。
- **自我反思模型 (Self-Reflection)**：記作 (M_{sr})，此為 Reflexion 框架的**關鍵創新點**。它接收稀疏的獎勵訊號（如二元成功/失敗狀態）、當前軌跡及其**持續記憶 `mem`**。然後生成**細緻且具體的口頭自我反思**，這些回饋比純量獎勵更具資訊量，並會儲存到記憶中供未來試驗使用。例如，在多步驟決策任務中，代理失敗時可推斷出哪個特定動作導致錯誤，並 verbally 建議應採取的正確動作，將此經驗儲存在記憶中以適應未來決策。
- **記憶 (Memory)**：Reflexion 實現長期與短期學習的關鍵組件。
    - **短期記憶**：軌跡歷史充當短期記憶，記錄最近的細節。
    - **長期記憶**：自我反思模型產生的輸出儲存為長期記憶，記錄從多次試驗中提煉出的重要經驗。 這兩種記憶協同為 Actor 提供上下文，使其決策既能參考最近細節，也能從過往教訓中學習。為符合 LLM 的最大上下文限制，記憶 `mem` 的大小被限制在一個最大值 (\Omega)，通常設定為 1-3 個經驗。

**Reflexion 的運作流程**是一個**迭代優化過程**。

1. **第一次試驗**：Actor 透過與環境互動生成軌跡 (\tau_0)。
2. **評估**：Evaluator 根據 (\tau_0) 計算出獎勵分數 (r_0)。
3. **反思**：Self-Reflection 模型分析 ({\tau_0, r_0}) 並生成摘要 (sr_0)，該摘要會儲存在記憶 `mem` 中。
4. **循環**：Actor、Evaluator 和 Self-Reflection 模型在循環中協同工作，直到 Evaluator 判斷 (\tau_t) 正確。每次試驗後，(sr_t) 都會附加到 `mem` 中。

### 4 實驗 (Experiments)

研究在決策制定、推理和程式碼生成任務上評估了 Reflexion 方法，並與強大基線進行比較。Reflexion 在 AlfWorld、HotPotQA 和 HumanEval 等任務上均顯著提升了性能。

#### 4.1 循序決策：AlfWorld (Sequential decision making: ALFWorld)

- **任務描述**：AlfWorld 是一個文字環境套件，挑戰代理在多步驟任務中解決問題，例如尋找、移動和操作物品。
- **代理配置**：使用 **ReAct** 作為動作生成器，因其在長軌跡決策中表現出色。
- **自我評估**：AlfWorld 任務需要自我評估。研究實施了兩種自我評估技術：
    - **LLM 自然語言分類**。
    - **手寫啟發式演算法**：若代理重複相同動作超過 3 次或動作數超過 30 次（低效規劃），則觸發自我反思。
- **實驗設置**：在基準線運行中，如果建議自我反思，則跳過並重設環境。在 Reflexion 運行中，代理使用自我反思找出錯誤、更新記憶並重設環境。記憶限制為最近 3 次自我反思。
- **結果**：
    - **ReAct + Reflexion 顯著優於 ReAct**，在 134 個任務中完成了 130 個。
    - **ReAct + Reflexion 在 12 次連續嘗試中學習解決了額外任務**。
    - 純 ReAct 方法在第 6 和第 7 次嘗試後性能提升停滯。
- **分析**：
    - 基準線 AlfWorld 軌跡的常見錯誤是代理**錯誤地認為擁有一件物品**，導致後續長軌跡中無法回溯錯誤。
    - Reflexion 透過自我反思，將失敗的長軌跡提煉成相關經驗作為「**自我提示**」，有效消除了這些情況。
    - 長期記憶在 AlfWorld 中有兩大幫助：
        1. **早期錯誤識別**：在長軌跡中可輕鬆識別早期錯誤，並建議新的動作或長期計畫。
        2. **徹底搜索**：代理可利用經驗記憶在多次嘗試中徹底搜索房間。
    - 學習曲線顯示，Reflexion 的學習過程在多次經驗中發生，從最初兩次嘗試之間的顯著提升，到接下來 11 次嘗試的穩步增加，最終達到近乎完美的表現。相比之下，純 ReAct 代理的幻覺率穩定在 22%，沒有長期恢復的跡象。

#### 4.2 推理：HotPotQA (Reasoning: HotpotQA)

- **任務描述**：HotPotQA 是一個基於維基百科的問答數據集，要求代理解析內容並在多個支援文件上進行推理。
- **代理配置**：
    - 為了**僅測試推理能力**，實施了 **Reflexion + Chain-of-Thought (CoT)** 的逐步問答（Q→A 和 Q, Cgt→A）。其中 Cgt 是來自數據集的真實情境，CoT 獲取 Cgt 以隔離推理行為。
    - 為了測試**整體問答能力**（包含推理和動作選擇），實施了 **Reflexion + ReAct** 代理，該代理可使用維基百科 API 檢索相關上下文並逐步推理答案。
    - CoT 使用 6 次少樣本提示，ReAct 使用 2 次少樣本提示，自我反思使用 2 次少樣本提示。
- **評估方法**：使用**精確匹配 (exact match)** 答案評分作為二元成功訊號。自我反思迴圈放大二元訊號，記憶大小為 3 次經驗。
- **結果**：
    - **Reflexion 在多個學習步驟中以顯著優勢超越所有基準線方法**。
    - 純 ReAct、純 CoT 和純 CoT (GT) 的實現**未能概率性地改進**任何任務，即第一次嘗試失敗的任務在後續嘗試中無法解決。
    - Reflexion 代理可收集經驗並重試失敗任務，直到連續 3 次失敗。
    - 儘管 CoT (GT) 由於可存取真實上下文而準確率較高，但仍有 39% 的問題無法正確推斷答案。**Reflexion 在無法存取真實答案的情況下，幫助代理將其準確率提高了 14%**。
- **分析 (消融實驗)**：
    - 為了隔離自我反思步驟對推理的優勢，進行了使用 CoT (GT) 作為基準線的消融實驗。
    - CoT (GT) 結合了 Chain-of-Thought 推理和真實上下文，測試代理在長上下文上的推理能力。
    - 加入情節記憶 (EPM) 後，包含最近的軌跡。
    - Reflexion 代理則實施標準自我反思步驟。
    - 結果顯示，**自我反思比情節記憶學習優勢高出 8% 的絕對提升**。這支持了**僅精煉的方法不如自我反思引導的精煉方法有效**的論點。

#### 4.3 程式設計 (Programming)

- **任務描述**：評估基準線和 Reflexion 方法在 **Python 和 Rust 程式碼撰寫**上的表現，涉及 MBPP、HumanEval 和新的 **LeetcodeHardGym** 數據集。
- **跨語言能力**：使用 **MultiPL-E**（一個小型編譯器集合）將 HumanEval 和 MBPP 的子集翻譯成 Rust，以證明 Reflexion 的程式碼生成實現是**語言無關的**。
- **LeetcodeHardGym**：引入了一個新的基準測試，包含 40 個在 GPT-4 預訓練截止日期（2022 年 10 月 8 日）之後發布的 Leetcode 困難級問題。
- **自我評估方式**：程式設計任務可使用**自我生成的單元測試套件**進行更紮實的自我評估。
    - 透過 **Chain-of-Thought 提示** 生成多樣化、廣泛且帶有自然語言描述的測試。
    - 篩選出語法有效的測試語句。
    - 從生成單元測試集合中採樣 (n) 個測試形成測試套件 (T)，其中 (n) 最多為 6 個。
- **實驗設置**：Reflexion 程式設計代理的學習迴圈設置與推理和決策代理相同，記憶最大限制為 1 次經驗。
- **結果**：
    - **Reflexion 超越所有基準線準確性，並在所有 Python 和 Rust 基準測試上設定了新的最先進標準，除了 MBPP Python**。
    - HumanEval (PY): Reflexion 91.0%，GPT-4 80.1%。
    - HumanEval (RS): Reflexion 68.0%，GPT-4 60.0%。
    - Leetcode Hard (PY): Reflexion 15.0%，GPT-4 7.5%。
- **分析**：
    - 程式碼生成代理的自我反思能力受其編寫**多樣化、全面測試**的能力限制。
    - **偽陽性** (FP: 單元測試通過但解決方案失敗) 問題：如果模型生成不穩定的測試套件，可能導致所有測試在不正確的解決方案上通過。
    - **偽陰性** (FN: 單元測試失敗但解決方案通過) 問題：如果模型生成不正確的測試套件，可能導致部分測試在正確的解決方案上失敗。
    - Reflexion 偏好**偽陰性**而非偽陽性，因為代理可能透過自我反思識別不正確測試並保留原始程式碼。
    - MBPP Python 上的 Reflexion 表現不如 GPT-4 的原因在於**內部測試執行產生的偽陽性標籤存在顯著差異**。MBPP Python 的偽陽性測試執行率為 16.3%，而 HumanEval Python 僅為 1.4%。
- **消融研究 (Ablation study)**：
    - 在 HumanEval Rust 的 50 個最難問題上測試了 Reflexion 複合方法中測試生成和自我反思的協同作用。
    - **省略內部測試生成和執行**：準確性降至 52% (基準線為 60%)。這表明代理在沒有單元測試指導下無法確定實現是否正確，導致持續進行有害編輯。
    - **省略自我反思貢獻** (省略自然語言解釋步驟)：性能**沒有提高**。儘管測試生成和編譯步驟能捕捉語法和邏輯錯誤，但實現修復並未反映這些指示。這暗示**沒有自我反思的盲目試錯除錯技術在困難任務上是無效的**。

### 5 侷限性 (Limitations)

Reflexion 作為一種利用自然語言進行**策略優化**的技術，雖然強大，但仍可能陷入**非最佳的局部最小值解決方案**。研究中，長期記憶被限制為具有最大容量的滑動視窗。

針對程式碼生成，測試驅動開發存在許多實際限制，難以指定準確的輸入-輸出對應，例如：

- **非確定性生成器函數**。
- 與 API 互動的**非純函數**。
- 依據硬體規格而**輸出變化的函數**。
- 調用**平行或並行行為**而難以預測的函數。

### 6 更廣泛的影響 (Broader impact)

大型語言模型正被廣泛應用於與外部環境（如網際網路、軟體、機器人等）和人類的互動。Reflexion 的工作潛力在於**強化這些代理程式，提升其自動化和工作效率**。然而，同時也**放大了這些代理被濫用時的風險**。這顯示此研究方向在**安全和倫理考量**方面需要更多努力。

另一方面，傳統強化學習因其**黑箱策略和優化設置**，在**可解釋性和對齊**方面一直面臨挑戰。論文提出的「**口頭強化學習**」有望解決這些問題，使自主代理程式**更具可解釋性和診斷性**。例如，在人類難以理解的工具使用情境下，可以透過監控自我反思來確保工具使用意圖的正確性。

### 7 結論 (Conclusion)

論文提出了 **Reflexion**，這是一種利用**口頭強化**來教導代理程式從過去錯誤中學習的方法。實證結果表明，Reflexion 代理程式透過**自我反思**，顯著優於目前廣泛使用的決策方法。未來，Reflexion 可進一步整合傳統強化學習中更進階的技術，例如自然語言中的價值學習或離線探索技術。

### 8 可重現性 (Reproducibility)

研究建議，在進行自主程式碼撰寫實驗時，應使用**隔離的執行環境**，因為生成的程式碼在執行前未經驗證。

### 附錄範例解析 (Appendix Examples Analysis)

附錄中的範例提供了 Reflexion 代理在不同任務中如何應用自我反思來學習和改進的具體案例。

#### D.1 決策制定完整範例：AlfWorld (Decision-making full example: AlfWorld)

這個範例展示了 Reflexion + ReAct 代理如何在 AlfWorld 環境中糾正**低效率規劃**造成的失敗。

- **第一次試驗**：代理的計畫是先找到馬克杯再找檯燈，但在執行過程中，它在找到馬克杯後，又重複尋找檯燈的動作，且沒有成功使用檯燈，最終任務失敗。
- **反思**：代理識別出錯誤在於**順序錯誤**，任務要求是用檯燈檢查馬克杯，而非尋找後再使用。它反思應該先找檯燈，然後再找馬克杯，並利用檯燈進行檢查。
- **第二次試驗**：代理依據反思調整策略，先前往書桌找到檯燈和馬克杯，然後成功拿起馬克杯並使用檯燈，最終任務成功。 這個例子說明，**Reflexion 幫助代理認識到並修正了其初始的行動計畫錯誤，使後續的行為更簡潔高效**。

#### D.2 推理範例：HotPotQA - 共同職業 (Reasoning example: HotPotQA - common profession)

這個範例展示了 Reflexion + Chain-of-Thought (CoT) 代理如何從推理錯誤中學習。

- **第一次試驗**：代理根據初步思考，錯誤地判斷 John Lanchester 和 Alan Dean Foster 的共同職業是「小說家和編劇」，導致答案不正確。
- **反思**：代理意識到自己**錯誤地假設兩人有相同的多重職業**。它反思未來應更專注於研究兩位作者的**個別背景**，以確保準確識別其職業，並考慮他們可能有多個共同職業的可能性。
- **第二次試驗**：代理根據反思，修正了對 John Lanchester 職業的理解，並準確地判斷出他們唯一的共同職業是「小說家」，最終任務成功。 這個例子強調了 **Reflexion 在糾正 LLM 推理過程中的假設錯誤，引導其進行更精確的事實檢索和判斷方面的作用**。

#### D.3 推理範例：HotPotQA - 戰役系列 (Reasoning example: HotPotQA - series of battles)

這個範例展示了 Reflexion + Chain-of-Thought (GT) 代理如何透過反思提供更全面和準確的答案。

- **第一次試驗**：代理在已知真實情境下，只提取了問題中提到的單一戰役名稱「白原市戰役」作為答案，但問題問的是「一系列戰役」，導致答案不正確。
- **反思**：代理意識到**未能提供足夠的上下文**，只提供了單一戰役名稱，而問題是關於「一系列戰役」。它反思未來應提供更多上下文，如**戰役活動的名稱（紐約和新澤西戰役）**，以及它是「一系列戰役」的事實，並確保包含日期和地點。
- **第二次試驗**：代理根據反思，將答案修正為「紐約和新澤西戰役」，最終答案正確。 這個例子顯示，**即使在有真實上下文的情況下，Reflexion 也能幫助代理糾正其對問題的理解偏差，從而提供更全面、更符合問題意圖的答案**。

#### D.4.1 (EPM) 思維鏈 + Reflexion 記憶消融範例：樂團成員 (Memory (EPM) ablation example: band members)

這個範例展示了在加入情節記憶 (EPM) 和自我反思後，代理如何修正對音樂家樂團成員身份的錯誤比較。

- **第一次試驗**：代理錯誤地計算了兩位音樂家（Jonny Craig 和 Pete Doherty）曾屬樂團的數量，導致答案不正確。
- **反思**：代理認識到其推理失敗是因為**沒有考慮到兩位音樂家過去和現在的樂團成員身份**。它反思未來應專注於研究兩位音樂家「過去和現在」的樂團，以確保準確比較。
- **第二次試驗**：代理根據反思，重新審視資料，發現 Jonny Craig 在過去曾是更多樂團的成員，最終給出了正確答案。 這個例子進一步證明了**自我反思加上情節記憶對於糾正 LLM 在信息檢索和比較中的細微錯誤的重要性**。

#### D.4.2 (EPM) 思維鏈 (GT) + Reflexion 記憶消融範例：學位領域 (Memory (EPM) ablation example: degree field)

這個範例說明了代理如何從混淆學位類別與特定領域的錯誤中學習，即使有真實上下文。

- **第一次試驗**：代理在已知真實上下文的情況下，將問題理解為詢問學位的「類別」（如「科學、工程和醫學」），而非「特定領域」（如「工程學」），導致答案不正確。
- **反思**：代理反思自己**誤解了問題**，將其視為學位類別而非特定學位。它從錯誤中吸取教訓，並在第二次試驗中更專注於問題，理解其在詢問「特定研究領域」。
- **第二次試驗**：代理根據反思，修正理解並給出正確答案「工程學」。 這個案例清晰地表明，**自我反思機制能夠幫助 LLM 克服對問題本身的誤解，即使提供了精確的上下文，也能引導其更精確地定位所需信息並給出精準答案**。

---