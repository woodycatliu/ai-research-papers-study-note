# **Memento: 在不微調LLM的情況下微調LLM代理**

## 1. 要解決的問題

目前的大型語言模型（LLM）代理面臨以下主要挑戰：

- **僵化或計算成本高昂**：現有方法要麼過於死板，依賴靜態、手工編寫的工作流程，無法在部署後整合新資訊或適應新情境；要麼需要對底層LLM模型參數進行梯度更新，這導致了高昂的計算成本。
- **連續適應與線上學習效率低下**：透過參數微調來更新LLM雖然能提供更靈活的行為，但在開放式場景中進行連續適應和線上學習時效率極低。這種方法通常需要大量的計算、資料，並有發生「**災難性遺忘**」的風險。
- **複雜任務的限制**：單純基於提示的方法雖然能直接嵌入工具呼叫，但處理長程任務往往需要多步工具呼叫。對於需要多步推理的複雜任務，LLM代理系統必須花費大量時間來收集訓練資料，並且額外依賴大量人工標註的問題。
- **記憶體管理的挑戰**：大多數現有系統僅是新增案例而不進行選擇性策劃，這可能導致「**記憶體過載問題**」（swamping problem），即檢索成本超過其實用價值。

> 論文的核心研究挑戰在於：「**如何在不產生微調底層LLM的昂貴成本下，建立能夠從不斷變化的環境中持續學習的LLM代理？**」

## 2. 如何解決問題

Memento 透過一個新穎的學習範式來解決這些問題，該範式允許自適應LLM代理進行持續學習，而無需微調底層LLM本身。其解決方案包括以下幾個關鍵點：

### 記憶體式線上強化學習
Memento 透過基於記憶體的線上強化學習實現低成本的持續適應。它將此過程形式化為一個**記憶體增強型馬可夫決策過程**（M-MDP），並配備一個神經案例選擇策略來指導行動決策。

### 靈感來自人類記憶
該方法借鑒了人類的記憶機制，將經驗編碼為情節性追蹤、提煉為抽象規則、透過多巴胺驅動的信用分配進行選擇性強化，並在遇到類似問題時透過案例或類比推理進行檢索。

### 規劃器–執行器架構
Memento 採用**規劃器–執行器**（planner–executor）架構，包括三個主要組件：
- **規劃器 (Planner)**：一個基於LLM的CBR代理，負責接收任務指令，從案例記憶體中檢索相關案例（任務、計畫、成功/失敗指示），並根據當前任務指令和檢索到的案例生成每個子任務的計畫。
- **工具輔助執行器 (Tool-enabled Executor)**：一個通用LLM，負責執行每個子任務，並利用外部工具來獲取資訊、處理多模態資料和進行推理。執行器透過 **Model Context Protocol (MCP)** 客戶端與外部工具互動。
- **情節性案例庫 (Episodic Case Bank)**：用於儲存過去的經驗軌跡（包括成功和失敗的案例），作為外在記憶體供代理使用，而不是僅依賴LLM固有的參數記憶體。

### 案例記憶體管理
案例記憶體是一個動態增長的案例庫，具有「寫入（Write）」和「讀取（Read）」操作，並有兩種變體：
- **非參數記憶體 (Non-Parametric Memory)**：
    - **寫入 (Write)**：直接將每次互動的狀態、行動和獎勵（$st$, $at$, $rt$）追加到案例庫中。
    - **讀取 (Read)**：透過計算當前狀態與過去狀態之間的語義相似性，檢索K個最接近的案例。這遵循了**案例式推理**（CBR）的範式，假設相似的問題應有相似的解決方案。
- **參數記憶體 (Parametric Memory)**：
    - **寫入 (Write)**：除了追加案例外，還在線上更新一個Q函數，以塑造檢索分佈。由於深度研究任務的獎勵訊號是二進制的 ($r \in \{0, 1\}$)，因此將訓練目標重新表述為交叉熵（CE）損失，作為二元分類問題來學習Q函數。
    - **讀取 (Read)**：由學習到的Q函數驅動，透過TopK操作選擇Q值最高的K個案例作為規劃參考，實現自適應案例選擇。

### Soft Q-Learning 優化
為了優化CBR策略，Memento 採用**最大熵強化學習**框架，並透過時序差分（TD）學習來學習Q函數。Q函數的近似透過基於核函數的估計來實現，維持一個包含狀態、檢索案例和Q值的情節性記憶體 $\mathcal{D}$。

### 全面的工具集
Memento 設計了一套可透過 **MCP** 協議訪問的工具，以支援開放式和複雜的任務：
- **外部資訊獲取**：整合搜尋引擎（如searxng、Google、Bing）和網頁爬蟲 (Crawl4AI)。
- **多模態異質資訊處理**：可從圖像、音訊、PowerPoint、試算表、壓縮檔、文本、程式碼、JSON、XML、Word 文檔和影片中提取資訊。
- **推理**：提供沙盒環境下的程式碼執行工具 (Python, Shell) 和數學計算工具。

### 性能表現
- 在 **GAIA** 驗證集上獲得 **87.88% Pass@3** 的頂級表現，並在測試集上獲得 **79.40%** 的成績。
- 在 **DeepResearcher** 數據集上達到 **66.6% F1** 和 **80.4% PM**，超越了最先進的訓練方法。
- 案例記憶體在分佈外（OOD）任務上帶來了 **4.7% 到 9.6%** 的絕對增益。
- 在 **SimpleQA** 上達到 **95.0%** 的準確度，有效減少了幻覺。
- 在 **Humanity’s Last Exam (HLE)** 上達到 **24.4% PM**，排名第二。
- 組件分析顯示，規劃和案例式推理為各基準測試帶來了顯著的提升。
- 觀察到「快速思考」的規劃器（例如 **GPT-4.1**）在產生簡潔且結構化的計畫方面，表現優於「慢速思考」的規劃器。

## 3. 未來展望與待改進之處

- **獎勵函數和記憶體更新的機率性**：目前論文將獎勵函數和記憶體更新建模為確定性過程，未來工作可以探索其機率性方面。
- **處理更複雜的L3任務**：儘管 Memento 在 GAIA 基準測試中表現強勁，但在需要更長推理時間和更複雜工具協調的 L3 任務上，仍存在挑戰。這可能需要進一步增強其規劃能力和工具協調機制。
- **提高領域知識整合**：對於像 **HLE** 這樣長尾、專家級的任務，僅靠工具使用或規劃並不能可靠地產生正確答案，這表明需要更好地將領域知識編碼到骨幹模型中或處理外部知識的方式。
- **優化案例庫的飽和與收益遞減**：在目前的模擬環境中，案例庫可能會迅速飽和，額外的迭代在達到幾次迭代後收益會遞減。這暗示在更複雜或真正的開放式環境中，可能需要更精細的記憶體管理策略或更長的學習階段，以避免記憶體過載並確保持續的性能提升。
- **擴展記憶體-MDP框架**：論文的成果激發了未來在記憶體基於 **MDP** 的深度研究任務上的工作，這可能涉及探索新的記憶體設計、學習演算法或代理架構。

> 總之，Memento 提供了一個可擴展且高效的途徑，用於開發能夠連續、即時學習的通用LLM代理，而無需梯度更新，從而推動機器學習在開放式技能獲取和深度研究場景中的發展。