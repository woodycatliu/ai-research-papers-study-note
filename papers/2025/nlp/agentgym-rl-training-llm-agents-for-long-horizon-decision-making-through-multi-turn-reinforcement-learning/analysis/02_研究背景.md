本研究背景植根於大型語言模型 (LLM) 應用從傳統聊天機器人向處理長時程現實世界任務的自主代理的快速發展。隨著 LLM 的巨大進步，研究領域逐漸意識到 LLM 代理需要透過探索和與環境互動來獲取知識和技能，這使得強化學習 (RL) 成為訓練這些代理的自然且重要的方法。

儘管 RL 在 LLM 推理等領域已有所進展，但現有大多數研究仍局限於單輪任務，缺乏模型與複雜環境進行多輪互動的能力。近期雖有嘗試將 RL 擴展至訓練具備多輪能力的 LLM 代理，但這些工作的任務複雜度和環境多樣性有限，且在優化穩定性和效率方面遭遇瓶頸，導致性能次優。

因此，當前研究社群面臨的關鍵問題是缺乏一個統一、端到端、互動式多輪 RL 框架，能夠在廣泛的現實世界場景和環境中，從零開始有效地訓練 LLM 代理，且無需依賴監督式微調 (SFT) 作為預處理步驟。為彌補這一研究空白，本文引入了 **AgentGym-RL** 框架，旨在透過 RL 訓練 LLM 代理進行多輪互動式決策，並提出 **ScalingInter-RL** 方法來解決探索-利用權衡問題並提高 RL 優化的穩定性。這項研究的重要性在於，它試圖提供一個靈活且可擴展的框架，以促進 LLM 代理在多樣化場景下自主學習和決策能力的發展。
