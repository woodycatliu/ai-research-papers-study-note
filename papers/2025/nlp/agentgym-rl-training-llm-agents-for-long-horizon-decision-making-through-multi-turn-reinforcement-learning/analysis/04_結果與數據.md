本研究透過在 AgentGym-RL 框架中進行廣泛實驗，驗證了其穩定性和有效性。實驗結果主要以圖表（圖 1、圖 6、圖 7、圖 8、圖 9）和表格（表 1、表 2、表 3、表 4、表 5、表 6）形式展示。

**主要研究發現和核心結果：**
*   **強化學習提升開源 LLM 代理智慧**：AgentGym-RL-7B 模型在五個不同場景中的平均成功率顯著超越其他開源模型，並與領先的閉源模型（如 GPT-4o 和 Gemini-2.5-Pro）相匹配甚至超越。這表明 RL 能有效彌合開源與專有模型在高級智慧任務上的性能差距。
*   **ScalingInter-RL 顯著且持續提升性能**：ScalingInter-RL 在多樣化環境和任務中持續優於基準模型。例如，在 WebArena 上實現了超過 10% 的提升，在 TextCraft 基準上超越基礎模型 30 分，達到最先進結果。圖 6 顯示了利用 AgentGym-RL 框架與 ScalingInter-RL 演算法可產生穩定、持續和實質性的獎勵提升。
*   **互動預算對訓練的影響**：較大的互動輪次預算（例如 10）在早期階段能實現更高性能，但隨訓練進程會迅速崩潰，顯示早期過度探索存在風險。相比之下，較短輪次（例如 5）限制了早期探索，但提供了更穩定的學習訊號，帶來更可靠的長期性能（圖 7）。ScalingInter-RL 透過逐步擴展互動時程，實現了更穩定和高效的訓練動態。
*   **訓練後和測試時計算的擴展潛力**：研究發現，在訓練後和測試時計算中進行策略性投資，比僅僅增加模型參數數量更具影響力。ScalingInter-RL 模型（7B 參數）在訓練後實現約 58.6% 的平均成功率，顯著優於 Llama3.1-70B（47%）和 Qwen2.5-72B（43%）等大模型（圖 1 右）。
*   **環境結構是強化學習效率的關鍵決定因素**：AgentGym-RL 在具有明確規則和因果關係的模擬環境（如 TextCraft、BabyAI、SciWorld）中提供了最顯著的性能飛躍。例如，在 SciWorld 中，得分從 1.50 提升到 50.50。而在更開放的環境（如 WebArena 和 Deep Search）中，性能提升相對溫和。儘管如此，這仍表明 RL 在那些可以透過探索輕鬆發現清晰回饋和成功路徑的環境中表現最出色。

**各環境的詳細任務表現：**
*   **網頁導航 (WebArena)**：ScalingInter-7B 總體準確率 26.00%，超越 GPT-4o (16.00%)，與 DeepSeek-R1-0528 (28.00%) 和 Gemini-2.5-Pro (28.00%) 相當。在購物和 CMS 任務中達到最先進熟練度，但在 "GitLab & Reddit" 子任務與 OpenAI o3 (34.00%) 和 o4-mini (36.00%) 仍有顯著差距（表 1）。
*   **深度搜尋**：ScalingInter-7B 總體得分 38.25，超越 GPT-4o (26.75) 和 Gemini-2.5-Pro (36.50)，與最強大的開源模型 DeepSeek-R1-0528 (40.25) 相當。在 NQ 任務中獲得最高的總體得分 (52.00)，在 TriviaQA 中與 GPT-4o 並列第一 (70.00)（表 2）。
*   **數位遊戲 (TextCraft)**：ScalingInter-7B 獲得 91.00 的傑出總體得分，使其穩居頂級專有和大型開源模型（93.00-94.00）之列。在深度 4 實現非零得分 (33.33)，並在最大複雜度下展示獨特的穩健性。AgentGym-RL-7B (89.00) 也表現出色，AgentGym-RL-3B (75.00) 遠遠超過了類似大小的模型，如 Qwen2.5-3B-Instruct (14.00)（表 3）。
*   **具身任務 (BabyAI)**：ScalingInter-7B 獲得 96.67% 的最高總體準確率，超越 OpenAI o3 (94.44%) 和 GPT-4o (86.67%) 等頂級模型，實現了最先進性能。在 GoTo、ActionObjDoor (AOD) 和 SynthLoc 中實現 100% 的完美得分（表 4）。
*   **科學場景 (SciWorld)**：ScalingInter-7B 以 57.00 的總體得分樹立了新的最先進，顯著超越所有開源和專有模型，包括次佳的專有模型 OpenAI o3 (41.50)。但在 "Chem-Mix" 子任務中所有被評估模型均獲得零分，表明在需要複雜科學推理和多步驟化學模擬的任務中存在系統性挑戰（表 5）。

**其他發現：**
*   **代理的測試時縮放**：所有模型都隨著輪次數的增加而表現出明顯的收益，驗證了 ScalingInter-RL 方法背後的見解。訓練有素的代理持續以顯著優勢超越基準模型（圖 8）。增加樣本數量會使 Pass@K 性能得到顯著提升（圖 9）。
*   **不同 RL 演算法的性能**：GRPO 在 TextCraft、BabyAI 和 Deep Search 基準上持續且實質性地優於 REINFORCE++。GRPO 的優越性歸因於其透過評估相對於學習基準的動作相對優點來緩解高變異性梯度，從而在複雜、低訊號環境中提供更穩定和穩健的梯度（表 6）。
*   **案例研究**：案例研究突出了基礎代理的缺點以及 RL 模型所取得的改進。RL 訓練的代理在導航、組合問題解決和網頁互動場景中克服了無效的行為循環，表現出適應性恢復策略和更系統的任務執行。但科學推理（例如 SciWorld 的 "Chem-Mix" 任務）和高效網頁互動（過度互動）中仍存在局限性，強調了需要進一步改進的領域（圖 10、圖 11、圖 12、圖 14、圖 15、圖 16）。
