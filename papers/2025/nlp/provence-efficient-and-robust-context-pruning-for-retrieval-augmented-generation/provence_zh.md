# Provence：用於檢索增強生成的高效且強健的上下文剪枝技術

作者：
Nadezhda Chirkova, Thibault Formal, Vassilina Nikoulina, Stéphane Clinchant
NAVER LABS Europe, Grenoble, France

[https://huggingface.co/naver/provence-reranker-debertav3-v1](https://huggingface.co/naver/provence-reranker-debertav3-v1)

---

**PROVENCE 範例**

**問題：** 你可以每天吃南瓜嗎？

**檢索到的上下文：**
南瓜在秋季最盛產。每天吃南瓜有助於減少發炎、增強免疫系統並促進眼睛健康。它還可能幫助降低血壓。然而，如果吃太多，高劑量的纖維可能會導致腹瀉。請繼續閱讀以了解南瓜的營養和健康益處。

**Provence** 會移除與使用者問題不相關的句子。
**相關性分數：** 0.96（Provence 還會輸出相關性分數）。

---

## 摘要

檢索增強生成（Retrieval-Augmented Generation, RAG）技術改善了大型語言模型（LLMs）的生成品質，但卻面臨長上下文帶來的運算開銷，以及不相關的檢索資訊傳播到生成回應中的問題。**上下文剪枝**（Context Pruning）透過在 LLM 生成前移除檢索上下文中的不相關部分，來解決這兩個問題。然而，現有的上下文剪枝方法存在限制，缺乏一個能在多種情境下都高效且強健的通用模型，例如上下文包含可變量的相關資訊、長度不一，或應用於不同領域時。

本研究旨在彌補這一鴻溝，介紹 **Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts)**，這是一個高效且強健的問答（Question Answering, QA）上下文剪枝器。它能針對給定的上下文動態偵測所需的剪枝量，並可即用（out-of-the-box）於多種領域。Provence 的三個關鍵要素是：將上下文剪枝任務視為**序列標記**（sequence labeling）、將上下文剪枝功能與**上下文重排序**（context reranking）整合、以及在**多樣化資料**上進行訓練。我們的實驗結果顯示，Provence 在多種領域和設定下，能夠在效能幾無下降的情況下進行上下文剪枝，且在標準 RAG 管線中幾乎不增加成本。我們也進行了更深入的分析和多項消融實驗，為未來訓練上下文剪枝器提供見解。

## 1. 簡介

檢索增強生成（RAG）已成為一種廣泛應用的範式，用於改善大型語言模型（LLMs）的事實性、歸因和適應性（Das et al., 2019; Asai et al., 2024; Seo et al., 2019; Lewis et al., 2020; Mallen et al., 2023a; Min et al., 2023）。透過使用檢索到的相關上下文來增強使用者的查詢，RAG 有助於避免生成不真實的資訊，並能提供生成答案所使用的參考資料。此外，使用特定領域的資料庫，可以讓模型存取並推理先前未知的知識，而無需對 LLM 進行微調。RAG 方法的另一個優點是其易於使用的即插即用（plug-and-play）架構（如 LangChain），使用者可以選擇最適合其特定案例的組件（檢索器、生成器 LLM、上下文粒度等），以最大化最終效能。

---

![](https://web-api.textin.com/ocr_image/external/3f8bc1b859e73129.jpg)

**圖 1：Provence 的推論（左）與訓練（右）流程示意圖。**

---

同時，RAG 的使用也增加了運算開銷，這源於檢索延遲和 LLM 處理增加的輸入長度。此外，檢索到的上下文中的不相關資訊可能會傳播到生成的答案中。這些問題可以透過開發更高效、更強健的 LLMs 來解決，例如進行架構變更以更有效地處理長上下文（Nawrot et al., 2024; Dao, 2024; Chevalier et al., 2023; Louis et al., 2025），或增加微調資料的多樣性以改善處理不相關上下文的能力（Lin et al., 2024）。然而，微調 LLM 可能會消耗大量資源，對於專有（閉源）的 LLMs 甚至不可能實施。

另一種解決方案是透過**剪枝**檢索到的上下文，移除與使用者查詢不相關的部分，從而減少上下文長度並加速生成。此類上下文剪枝模組可以以即插即用方式與任何生成器 LLM 搭配使用，既方便又提高了 RAG 管線的透明度。

儘管已有針對 RAG 上下文剪枝的初步研究，但現有解決方案中，沒有一個模型是真正可以即用（out-of-the-box）於實際應用。首先，許多方法是為簡化情境設計的，例如：假設每個上下文只有一個句子與輸入查詢相關（Wang et al., 2023; Xu et al., 2024），或壓縮率是固定的（Jiang et al., 2023; Pan et al., 2024）。但在實踐中，上下文可能包含不同比例的相關資訊（從無到全部），剪枝器需要以適應性方式偵測這一點。其次，許多研究引入的上下文剪枝器效率不足，無法實際應用。這包括使用數十億參數的 LLMs 作為基礎模型（Jiang et al., 2024; Pan et al., 2024; Wang et al., 2023），或設計需要依序自迴歸生成最終上下文的**抽象式**（abstractive）壓縮器（Wang et al., 2023; Xu et al., 2024）。我們認為，更實用和高效的作法是微調一個小型模型，例如 DeBERTa (He et al., 2021b,a)，作為**抽取式**（extractive）剪枝器，亦即使用一個輕量級的預測頭來選擇相關的上下文部分。第三，大多數現有研究都是針對單一資料集訓練上下文剪枝器，沒有考慮或測試剪枝器在多種資料領域上的強健性。

表 1 總結了現有方法的特性，並顯示沒有一個方法能滿足所有列出的標準。表格還包括剪枝粒度（token 級 vs. 句子級）的維度。本研究專注於**依查詢的句子級剪枝**，它會移除與生成答案無關的語義單元（句子）。另一種方法是 token 級剪枝，通常不依賴查詢，移除低階的語法單元，如冠詞或感嘆詞。這兩種方法是互補的，可以潛在地結合使用。

- token級剪枝： 模型移除對語義貢獻小的 token, 如：的、個、量詞甚至是感嘆詞。token 級剪枝不考慮整個句子的意思，而是根據每個 token 重要性判斷。
  著重於移除**單獨的詞彙或符號**，不考慮上下文。像是一個文件編輯器中的「刪除不必要單字」功能。
- 句子級剪枝：模型分析整個句子與主要查詢或是任務之間的關聯性，移除與核心主題無關的句子。
  著重於移除**完整的句子**，會考慮與核心主題的相關性。像是一個文件編輯器中的「刪除無關段落」功能。

---

**表 1：現有上下文剪枝方法的分析。紫色/橙色高亮顯示實用/不那麼實用的解決方案。**

| 方法                  | 查詢依賴性 | 粒度      | 輸出           | 類型     | 基礎架構               | 多領域測試 | 模型發布 |
| ------------------- | ----- | ------- | ------------ | ------ | ------------------ | ----- | ---- |
| Selective Context   | 否     | token 級 | % of tokens  | extr.  | Llama-7B / GPT2    | 是     | 是    |
| LLMLingua           | 否     | token 級 | % of tokens  | extr.  | Alpaca-7B / GPT2   | 是     | 是    |
| LongLLMLingua       | 是     | token 級 | % of tokens  | extr.  | Llama-2-7B-chat    | 是     | 是    |
| LLMLingua2          | 否     | token 級 | % of tokens  | extr.  | RoBERTa / mBERT    | 是     | 是    |
| RECOMP extr.        | 是     | 句子級     | 𝑘sentences  | extr.  | BERT               | 否     | 是    |
| RECOMP abstr.       | 是     | 句子級     | ⩾0 sentences | abstr. | T5-L               | 否     | 是    |
| FilCo               | 是     | 句子級     | 1 sentence   | abstr. | T5-XL / Llama-2-7B | 否     | 否    |
| COMPACT             | 是     | 句子級     | ⩾0 sentences | abstr. | Mistral-7B         | 否     | 是    |
| **Provence (ours)** | 是     | 句子級     | ⩾0 sentences | extr.  | DeBERTa            | 是     | 是    |

---

為了解決上述限制，我們引入了 **Provence (Pruning and Reranking Of retrieVEd relevaNt ContExt)**，一種用於訓練可適應、高效且強健的句子級問答上下文剪枝器的方法，它可以即用（out-of-the-box）於各種領域和設定。為達成此目標，我們將上下文剪枝任務公式化為**二元序列標記**（binary sequence labeling），由剪枝器預測的二元遮罩（binary mask）來決定與查詢相關的句子（從零個到全部），並在多樣化的資料集上從一個輕量級的 DeBERTa 模型開始訓練我們的剪枝器。

此外，我們注意到上下文剪枝和重排序（RAG 管線中的第二步）有很強的相似性。因此，我們提出將這兩個模型合而為一，完全消除了上下文剪枝在 RAG 管線中的成本。

具體來說，我們的貢獻如下：
- 我們提出了一種訓練可適應、強健且高效的問答上下文剪枝器的方法，並發布了我們訓練好的模型。該方法的三個關鍵要素是：將上下文剪枝視為序列標記、在單一模型中整合上下文剪枝與重排序，以及在多樣化資料上進行訓練。
- 我們在多個問答領域上測試了 Provence，展示了其即用（out-of-the-box）能力，能在效能幾無下降的情況下剪枝上下文，且幾乎不增加成本，大幅超越基線方法。我們還展示了 Provence 能夠偵測上下文中任意位置的相關句子數量，以及其對不同上下文長度的強健性。
- 我們進行了多個消融實驗，以展示哪些技術對於訓練強健的上下文剪枝器至關重要，為未來上下文剪枝器的開發提供見解。

**定義：**
一個典型的 RAG 管線包含：(0) 使用者問題或查詢；(1) 一個資料庫（datastore），即一個用於檢索的文件（文本片段）集合；(2) 一個高效的檢索器（retriever），通常是雙編碼器（dual-encoder）模型，能夠從大型資料庫中快速檢索（獨立編碼查詢和段落）；(3) 一個更昂貴的**交叉編碼器重排序器**（cross-encoder reranker），它會進一步縮小和重新排序檢索到的段落集合（交叉編碼意味著將段落與查詢一起編碼）；(4) 一個**生成器 LLM**，它根據使用者查詢和相關段落輸出最終回應。這樣的管線可以表示為：`檢索 >> 重排序 >> 生成`。

上下文剪枝可以在生成之前加入，即：`檢索 >> 重排序 >> 剪枝 >> 生成`。在我們的研究中，我們還提出將上下文剪枝整合到重排序中，這是一個 RAG 中已有的重要組件（Rau et al., 2024a）：`檢索 >> 重排序+剪枝 >> 生成`。這使得上下文剪枝的成本幾乎為零。

## 2. 相關工作

**上下文剪枝。** RECOMP (Xu et al., 2024) 專注於 RAG 的上下文剪枝，提出了抽取式和抽象式兩種剪枝器。抽取式 RECOMP 方法獨立編碼上下文中的句子，然後選擇嵌入與查詢嵌入最接近的頂部句子。這種方法由於獨立處理句子和查詢，限制了對上下文的理解。該方法還需要指定保留多少句子作為超參數，這在實際應用中通常是未知的，並且應該依據每個特定的段落而定。抽象式 RECOMP 則透過在 GPT-3.5 生成的銀標摘要上進行訓練，來總結與查詢相關的關鍵資訊（包括沒有相關資訊的情況）。然而，它需要低效的自迴歸生成最終上下文，並可能產生輸入上下文中不存在的事實幻覺。

FilCo (Wang et al., 2023) 類似地建議自迴歸生成上下文，但其訓練目標是抽取式目標，即從上下文中根據三個標準之一選擇一個句子。其缺點同樣是效率低，並簡化了假設每個上下文只有一個相關句子的前提。近期的方法 COMPACT (Yoon et al., 2024) 也建議以自迴歸方式生成過濾後的上下文（因此效率低下），並引入了一種迭代方法，用於在處理新檢索到的段落部分後逐步更新相關上下文。與所有這些努力相反，Provence 以抽取式和高效的方式，動態偵測上下文中的相關資訊量（從零個到所有句子）。此外，我們提出了一種將上下文剪枝整合到重排序器中的新穎方法。

與我們的研究同時，DSLR (Hwang et al., 2024) 透過使用現有的重排序器，將句子與查詢逐一編碼來執行抽取式的句子級剪枝。與 Provence 類似，DSLR 保留分數高於某個門檻的句子，並保持其原始順序。然而，與 Provence 相比，由於獨立處理句子，DSLR 無法保留語義上相互關聯的句子群組。

另一條正交的研究路線提出了抽取式的 token 級剪枝器。LLMLingua (Jiang et al., 2023) 和 Selective Context (Li et al., 2023) 使用 LLMs 來移除生成機率高的 token，且與查詢無關。LLMLingua2 (Pan et al., 2024) 則是一個基於小型 BERT 的模型，經過微調以消除冗餘 token，同樣與查詢無關。LongLLMLingua (Jiang et al., 2024) 提出了基於對比困惑度（contrastive perplexity）的、依查詢的 LLM 級 token 剪枝。這些方法移除 token 的方式不會破壞 LLM 對上下文的理解，因此它們無法移除上下文的語義部分。LLMLingua 模型在介面中也有許多難以在實踐中調整的超參數。然而，這些方法也可以與句子級剪枝結合使用。

**檢索粒度。** 另一種替代上下文剪枝的方法是將資料庫內容重新格式化為原子單元，例如像 Dense-X 檢索 (Chen et al., 2024c) 中的命題，或**去上下文化**（decontextualized）的句子 (Choi et al., 2021)。此類預處理成本高昂，且可能導致某些資訊丟失。

**段落過濾。** 另一條相關且正交的研究路線專注於過濾整個段落，如果它們被認為與給定問題無關；這種方法可以與 Provence 直接結合。一個簡單的方法是在（重）排序分數上設定一個門檻。LongLLMLingua 根據問題在給定段落下的機率來重排序段落。(Yoran et al., 2024) 使用自然語言推理模型來過濾掉不蘊含問題-答案對的段落，但報告稱這種方法有時也會過濾掉相關的段落。

**改善 LLM 的上下文處理。** 雖然上下文剪枝器旨在移除與使用者查詢不相關的上下文部分，另一條研究路線則旨在讓 LLMs 更高效、更有效地處理上下文。高效的上下文處理可以透過高效的注意力實作（Dao, 2024; Anagnostidis et al., 2023）、KV 快取壓縮（Nawrot et al., 2024）、並行編碼檢索到的段落（Zhu et al., 2024），或將上下文壓縮為一個或多個上下文嵌入（Chevalier et al., 2023; Ge et al., 2024; Rau et al., 2024b; Louis et al., 2025）來實現。其他工作則旨在透過在訓練或微調期間將 LLMs 暴露於帶雜訊的上下文中，使其更加強健（Izacard et al., 2022; Lin et al., 2024）。所有這些方法通常需要對 LLM 進行修改，這可能會使得應用於任意選定的 LLM 變得複雜。

## 3. Provence

我們提出的方法的高階概覽如圖 1 所示。我們的第一個貢獻是將上下文剪枝問題視為一個**序列標記任務**。我們微調一個 DeBERTa 模型來編碼查詢-上下文對，並輸出二元遮罩，用於過濾不相關的上下文部分。訓練用的標籤由 Llama-3-8B-Instruct (AI@Meta, 2024) 生成；我們稱之為**銀標**（silver labels），因為它們是自動生成的。這種方法解決了現有上下文剪枝器的幾個限制：(1) 模型的建構使其能夠處理上下文中不同程度的雜訊，並選擇適當的剪枝比例；(2) 查詢與上下文句子一起編碼（交叉編碼），提供更豐富的表徵，相比之下，抽取式 RECOMP 獨立編碼查詢和上下文句子；(3) 使用輕量級編碼器使得我們的方法比基於 LLM 或抽象式方法更有效率。

我們的第二個貢獻是將重排序和上下文剪枝合而為一，而不是將它們視為 RAG 管線中獨立的步驟。在 Provence 中，重排序和剪枝可以在一個單一的前向傳播步驟中完成，從而消除了上下文剪枝帶來的運算開銷，使得 Provence 幾乎是「免費」的。

**訓練資料。** 我們的訓練方法需要一組訓練問題和一個檢索資料庫。具體來說，我們依賴 MS MARCO 文件排序集合的訓練集，其中包含 370k 個查詢 (Nguyen et al., 2016)。MS MARCO 集合是一個包含 3.2M 個從網路上爬取文件的多領域資料庫，這對於最終模型的領域強健性是必需的，並經常用於訓練檢索器和重排序器。我們還考慮了包含 87k 個查詢的 Natural Questions 訓練集 (Kwiatkowski et al., 2019)。

**資料處理。** 我們透過將 MS MARCO 文件分割成由 $N$ 個連續句子組成的段落來建立檢索資料庫，$N$ 是一個介於 1 到 10 之間的隨機整數。這是為了使剪枝器對可變的檢索上下文長度具有強健性。我們還在每個段落前加上頁面標題。對於每個問題，我們使用一個強大的檢索管線（Rau et al., 2024a），包括 SPLADE v3 檢索器 (Lassance et al., 2024) 和 DeBERTa-v3 重排序器 (Lassance & Clinchant, 2023)，來檢索排名前 5 的相關段落。由於檢索的不完美，由此產生的檢索段落集合在與問題的相關性或不相關性方面自然是多樣化的。

**銀標籤生成。** 給定一個問題和一個檢索到的段落（上下文），我們將段落分割成句子¹，並提示 Llama-3-8B-Instruct 選擇與給定問題相關的句子。一種方法是使用簡單的提示，例如「輸出與給定問題相關的句子索引」。然而，我們決定利用強大 LLMs 實際回答問題並引用相關上下文句子的能力。因此，我們指示 LLM **僅**使用給定上下文中提供的資訊來回答問題，如果沒有提供有用的資訊，則輸出「沒有答案」。我們還指定了一個易於解析的引用格式 `[i]`，並用相同的標記來編號上下文中的句子。我們的提示可以在附錄——表 6 中找到；我們使用貪婪解碼並使用正規表達式解析引用的句子。我們還在消融研究中比較了不同的提示策略。

我們發現 Llama-3-8B 在大多數情況下能夠僅根據給定的上下文進行回答，並在約 90% 的時間內產生引用。我們過濾掉那些沒有產生引用且輸出中不包含「沒有答案」的情況，因為這些情況下上下文實際上包含相關資訊，但 LLM「忘記」引用了。最終的標籤分佈（每個上下文選定的句子數量及其位置）顯示在附錄——圖 5 中。

**Provence 的訓練。** 我們的上下文剪枝器接收問題和檢索上下文的串聯作為輸入，並輸出每個 token 的二元標籤，表示每個 token 是否應包含在選定的上下文中。在第 4.4 節（消融實驗）中，我們還考慮了一種方法，即在每個句子開頭插入一個特殊 token，並根據這些 token 的表徵來預測每個句子的標籤。我們將 Provence 訓練為一個二元的 token 級分類器，其真實標籤來自銀標籤資料，該模型可以作為獨立的剪枝器使用，即：`檢索 >> 重排序 >> Provence（獨立） >> 生成`。

**整合壓縮與重排序。** 我們注意到交叉編碼器重排序器（Nogueira & Cho, 2020）與 Provence 共享相同的架構和輸入（問題-段落對）。此外，上下文剪枝（選擇上下文中對生成答案有用的部分）的任務，本質上與重排序（評估上下文與問題的相關性）有相似之處，我們假設這兩個相關任務之間存在知識轉移的可能性。因此，我們建議將這兩種方法統一到一個模型中，並帶有兩個不同的任務頭。更具體地說，重排序頭為 BOS token 輸出一個純量預測，而剪枝頭則為段落 token 輸出 token 級別的預測，如圖 1 所示。

為了簡化訓練，我們建議在一個預訓練的重排序器上進一步微調我們的標記目標，同時添加一個排序「正規化器」（ranking regularizer）來保留初始的重排序能力。該正規化器是針對初始重排序器分數的均方誤差（Mean Squared Error, MSE）損失。這可以被視為一個簡單的點式分數蒸餾過程，其中初始模型充當教師模型，這種方法在資訊檢索中已被證明非常有效 (Hofstätter et al., 2021)。最終的損失函數如下：

$$
\mathcal {L}=\sum _{n=1}^{N}\left\{\sum _{k=1}^{L_{n}}\log P\left(y_{n,k}\vert z_{n,k}\right)+λ\left(s_{n}-z_{n,0}\right)^{2}\right\}\tag{1}
$$

其中 $z_{n}=\text {Provence}\left(x_{n}\right)$，$N$ 是資料點（查詢-段落對）的數量，$x_{n}$ 是 $L_{n}+1$ 個輸入 token 的序列（串聯的查詢、段落和位於第 0 位置的 BOS），$z_{n}$ 是模型輸出的 $L_{n}+1$ 個預測序列，$y_{n}$ 是用於上下文剪枝的 $L_{n}$ 個目標二元標籤序列，$s_{n}$ 是教師分數（初始重排序器），$z_{n,0}$ 是從 BOS 表徵預測的排序分數。

對於統一模型，重排序和上下文剪枝只需要編碼器的一個前向傳播步驟，即 `檢索 >> Provence（含重排序） >> 生成`，這使得上下文剪枝的執行時間成本幾乎為零。

**使用 Provence 進行推論。** 在推論時，我們將串聯的問題和檢索段落送入 Provence，它會輸出每個 token 是否應包含在最終上下文中的機率，以及在統一模型中的段落分數。我們簡單地使用一個門檻 $T$ 來將 token 機率二值化（保留或不保留），這直接影響壓縮率。如實驗部分所示，門檻的選擇通常可以在不同的資料集之間轉移，使得模型能夠靈活地即用（out-of-the-box）於各種問答應用²。

我們注意到，儘管標記任務是句子級的，我們的模型輸出的是 token 級的預測。我們發現在最終上下文中包含 token 的機率自然會在句子級別上聚集，這得益於訓練中使用的句子級目標。然而，在極少數情況下，我們仍可能選中部分句子。為避免這種現象，我們採用了「句子四捨五入」（sentence rounding）程序：對於每個句子，我們檢查保留的 token 比例（`predicted label=1`），並僅當該比例高於 0.5 時才選擇整個句子。

## 4. 實驗

### 4.1. 實驗設定

**Provence 訓練細節。** 我們使用 PyTorch (Paszke et al., 2019) 和 HuggingFace transformers (Wolf et al., 2020) 在第 3 節描述的資料上訓練 Provence。我們使用 DeBERTa-v3 (He et al., 2021a) 作為預訓練模型來訓練獨立的 Provence。對於統一方法，我們從一個已經訓練好的交叉編碼器開始訓練，該模型也基於 DeBERTa-v3 (Lassance & Clinchant, 2023)。請注意，在後者中，我們從其微調版本初始化排序頭，並從頭開始訓練單獨的剪枝頭。

在初步實驗後，我們將學習率設為 $3 \times 10^{-6}$，批次大小為 48，並訓練模型一個 epoch。對於聯合訓練，剪枝和重排序之間存在輕微的權衡。我們將重排序正規化係數 $\lambda$ 設為 0.05，這是一個在 MS MARCO 開發集上不會顯著降低重排序效能的最小值。

**評估資料集。** 我們在多樣化的問答資料集上測試 Provence。首先，我們考慮了依賴 Wikipedia 資料庫的常用資料集：Natural Questions (Kwiatkowski et al., 2019)、TyDi QA (Clark et al., 2020)、PopQA (Mallen et al., 2023b)（均為單跳問題），以及 HotpotQA (Yang et al., 2018)（多跳問題）。其次，我們考慮了來自不同領域資料庫的資料集：BioASQ (Nentidis et al., 2023)（生物醫學問題，以 Pubmed 為資料庫）、SyllabusQA (Fernandez et al., 2024)（關於教育課程事務的問題，以課程大綱為資料庫），以及 RGB (Chen et al., 2024b)（關於新聞的問題，以 Google 搜尋的新聞為上下文）。更多細節請見附錄 A。

**評估設定。** 我們使用 BERGEN (Rau et al., 2024a) 這一用於 RAG 的基準測試函式庫，並採用其推薦的實驗設定。對於每個查詢，我們使用強大且強健的檢索管線：SPLADE-v3 >> DeBERTa-v3 重排序器，來檢索排名前 5 的相關段落（RGB 除外，其 Google 搜尋的段落已提供）。然後，我們將前面加上相關文件（完整長度或剪枝後）的查詢傳入 Llama-2-7B-chat (Touvron et al., $2023)^{3}$ 以生成答案；其他檢索器-生成器設定在附錄中報告。每個評估資料集都帶有簡短的關鍵字答案，我們使用它們來透過基於 LLM 的評估（LLMeval in Rau et al., 2024a）來評估回應；基於匹配的指標也將在附錄中報告。我們還將壓縮率量化為被剪枝的上下文部分。

我們將 Provence 與表 1 中列出的公開可用上下文剪枝模型進行比較，除了 LLMLingua 和 Selective Context，因為它們已被證明效能不如 LLMLingua2 (Pan et al., 2024)。對於所有上下文剪枝器（抽象式 RECOMP 除外，它不適用），我們強制選擇第一個（標題）段落句子，以避免生成器對上下文的理解產生歧義。對於抽取式 RECOMP，我們使用在 NQ 上訓練的模型，考慮使用 top 1/2/3 句子，並在每個句子前加上段落標題。對於 LLMLingua 系列，我們改變壓縮率為 {0.25, 0.5, 0.75}，並使用官方儲存庫提供的程式碼⁴。我們為 LLMLingua2 使用 XLM-RoBERTa 模型。對於 Provence，我們使用 $T=0.1$ 和 $T=0.5$。我們還將我們的方法與基於相同重排序器（即 DeBERTa-v3）的 DSLR 進行比較。

### 4.2. 主要結果

上下文剪枝器通常僅在有限的領域資料上進行測試，例如使用 Wikipedia 資料庫，而我們工作的一個重要方面是在一系列問答領域上評估上下文剪枝。圖 2 報告了在各種問答資料集和上下文剪枝方法中，壓縮（效率）和 LLM 評估效能（品質）之間的權衡。我們選擇為每個資料集報告一張圖，以更好地評估現有解決方案的帕雷托前緣（Pareto front），而不是在同一張表格中比較具有不同壓縮率的方法。附錄中的圖 7 進一步報告了基於匹配的指標的類似結果，附錄表 11-13 則展示了不同方法的上下文剪枝範例。

首先，我們觀察到，Provence 在相近的壓縮率下，達到了所有剪枝方法中的最高效能。其次，值得注意的是，Provence 的表現優於需要更多運算的方法，如 LLMLingua 模型，這表明效率並未以有效性為代價。此外，Provence 是唯一能夠在所有資料集上實現高壓縮率而效能卻沒有（或只有微不足道的）下降的方法。不僅如此，對於某些資料集，例如 PopQA，使用 Provence 進行剪枝甚至能因過濾掉上下文雜訊而帶來效能提升。

**門檻的影響。** 上下文剪枝器即用（out-of-the-box）能力的一個重要方面是，需要花多少精力來選擇合適的超參數值。對於 Provence 來說，這僅僅是設定剪枝門檻 $T$。在圖 2 中（對於 $T=0.1$ 和 $T=0.5$），我們觀察到 Provence 的剪枝率會根據資料集自動在 50% 到 80% 之間變化，這表明相同的 $T$ 值適用於所有考慮的領域，使得 Provence 對於超參數的選擇具有強健性。如有需要，使用者仍然可以針對其資料集和/或需求進一步調整。我們注意到，某些模型將所需的壓縮率指定為超參數，例如 LLMLingua 模型或抽取式 RECOMP（透過 top-N 句子）。雖然這看似方便估計推論成本，但「最佳」壓縮率（在不損失效能的情況下）對於每個特定的問題-上下文對都是獨特的。因此，使用門檻作為超參數更適合這項任務。我們還實驗了在抽取式 RECOMP 中指定門檻（在同一圖中顯示），發現它通常會導致較低的效能（與 top N 相比）。原因是不同的查詢有不同的相似性分數範圍。

**效率。** 我們在效率方面將 Provence 與其他剪枝方法進行比較。表 2 報告了不同剪枝方法所需的壓縮時間和 MFLOPS⁵。正如預期，LongLLMLingua（基於 LLama-2-7B-chat）是最慢的上下文剪枝器。RECOMP abstr. 所需的 MFLOPS 少於 Provence，但其自迴歸性質使其在實踐中速度較慢⁶。請注意，在統一模型的情況下，剪枝幾乎是免費的，因為它是重排序步驟的一部分。表 3 報告了使用 Provence 模型壓縮後（約 50% 壓縮率）的生成加速效果。所有運行均在單個 Tesla V100-SXM2-32GB GPU 上使用 vllm (Kwon et al., 2023) 執行。在大型批次下，我們系統地觀察到推論加速 2 倍，而較小的批次則導致較低的增益（特別是對於較小的模型）。我們假設這主要是由於 CPU/GPU 通訊瓶頸，掩蓋了壓縮帶來的推論增益。

### 4.3. 分析

在本節中，我們進行更精細的評估，以更好地理解 Provence 的特性。

**對上下文中相關資訊位置的強健性。** 我們設計了一個「大海撈針」（needle-in-the-haystack）實驗，讓我們能夠在一個簡單的玩具範例上檢查 Provence 的效能，並評估其對輸入上下文中相關資訊位置的強健性。我們撰寫了 5 個問題和答案⁷，並將它們插入到從維基百科檢索到的 100 個隨機段落樣本中。理想情況下，Provence 應該只選擇「針」句子並過濾掉所有其他句子（圖 3，左）。我們考慮了兩種設定：插入 1 個句子和 2 個句子作為「針」。我們觀察到 Provence 在大多數情況下都能正確選中「針」，這顯示了其偵測相關資訊的能力，無論其在上下文中的位置如何。

---

![](https://web-api.textin.com/ocr_image/external/b5b3676f25fdbde9.jpg)

**圖 3：分析上下文中的句子。**
(左)「大海撈針」測試，顯示了 Provence 選擇「針」句子並過濾掉其他句子的能力，不論其在上下文中的位置如何。
(中) Provence 剪枝句子數與銀標籤預測器（LLAMA-3-8B-Instruct）選定的句子數的比較。熱圖按行歸一化：位置 (i, j) 的單元格表示被銀標籤預測器剪枝為 i 個句子的上下文中，有多少百分比被 Provence 剪枝為 j 個句子。
(右) 在不同上下文粒度的設定下測試 Provence。所有實驗均使用統一的 Provence，T=0.1。

---

此外，我們還想檢查 Provence 是否能夠動態偵測上下文中相關句子的數量，我們將 Provence 預測的句子數量 𝐿 與銀標籤神諭（silver oracle）的句子數量進行比較，因為銀標籤是透過 Llama-3-8B-Instruct 生成的，它會根據上下文動態選擇。圖 3（中）顯示了剪枝器對 Natural Questions、Syllabus QA、HotpotQA 和 BioASQ 的表現。我們觀察到，在大多數情況下，Provence 選擇的句子數量接近銀標籤神諭的值，適用於所有考慮的資料集。相比之下，抽取式 RECOMP 則總是會選擇一個預先指定的句子數量。

**對上下文粒度的強健性。** 圖 3（右）顯示了 Provence 在兩個資料集上的表現，這些資料集的 Wikipedia 資料庫由不同粒度的上下文組成。在此，每個資料庫都是透過將 Wikipedia 頁面分割成 $N$ 個句子，其中 $N \in \{2, 6, 10\}$，或 100 個詞，並在每個區塊前加上頁面標題。Provence 在所有情況下都表現出高效能，剪枝後上下文的效能與使用原始上下文獲得的效能相近。正如預期的那樣，上下文越長，壓縮率越高。

**重排序的有效性。** 表 4 比較了我們的重排序基線與統一 Provence 之間的重排序效能，後者的訓練從前者開始。我們可以看到，我們的聯合訓練程序（同時處理剪枝和排序任務）使得訓練一個上下文剪枝器成為可能，同時保留了初始的重排序能力。我們還納入了一個在 NQ 上以類似條件訓練的模型作為比較點。總體而言，結果相似，這進一步凸顯了 Provence 對於訓練資料的強健性。我們在第 4.4 節（消融實驗）中進一步討論了這些方面。

**在不同設定下的適用性。** 圖 8（附錄）展示了 Provence 在可變的檢索器-生成器設定下的適用性，取得了與圖 2 中報告的相似結果。

---

**表 4：對 SPLADE-v3 檢索到的前 50 份文件進行重排序的有效性。**
DeBERTa-v3 是「基線」（Provence 的初始化點，我們旨在保留其效能）。我們報告了兩個 RAG 資料集（NQ 和 HotpotQA）的 R@5、MS MARCO 段落（開發集）的 MRR@10、TREC DL’19 (Craswell et al., 2020) 的 nDCG@10，以及 BEIR 基準測試中 13 個開放資料集（Thakur et al., 2021）的平均 nDCG@10。附錄中的表 7 報告了完整的結果。

| 模型 | NQ | HotpotQA | MS | TREC19 | BEIR |
|---|---|---|---|---|---|
| DeBERTa-v3 | 83.0 | 70.4 | 40.5 | 77.4 | 55.4 |
| Provence | 84.4 | 70.5 | 40.6 | 77.2 | 55.9 |
| Provence (NQ) | 84.5 | 70.3 | 40.2 | 77.5 | 55.1 |

---

### 4.4. 消融實驗

在本節中，我們分析了 Provence 開發過程中的各種設計選擇，為未來訓練上下文剪枝器提供見解（結果如圖 4 所示）。本節中的所有模型都是獨立的上下文剪枝器，以相同的參數更新量進行訓練。

**模型大小。** 我們首先觀察到 DeBERTa-large 略微增加了壓縮率，與 DeBERTa-base 相比。所有其他消融實驗都為了效率而從 DeBERTa-base 模型進行微調。請注意，最終的 Provence 是從 DeBERTa-large 模型（或其等效的重排序器）訓練而來的。

**資料混合。** 我們比較了在 NQ (87k 查詢)、下採樣到相同大小的 MS MARCO，以及完整的 MS MARCO (370k 查詢) 上進行訓練。我們觀察到，使用 MS MARCO 類型的資料在查詢數量相同時，表現與在 NQ 上訓練相似，我們還發現使用更大的資料（即完整的 MS MARCO）可以改善結果。我們的最終模型是在完整的 MS MARCO 上訓練的，為了效率，進一步的消融實驗則是在 NQ 資料上進行的。

**標籤策略。** 如第 3 節所述，我們可以訓練剪枝器執行 token 級別的標記（在推論時進行句子四捨五入），或者執行句子級別的標記。在前一種情況下，句子表徵更豐富，但模型也需要學習為一個句子內的所有 token 輸出相似的預測。在後一種情況下，句子內容必須用一個單一的嵌入來表示，這可能會限制表徵的表達能力。在實踐中，我們觀察到效能相近，而 token 級策略在某些資料集上略優於句子級策略。在所有其他實驗中，我們都使用 token 級策略。

**神諭提示。** 我們比較了三種用於提示神諭 LLM 生成銀標籤的選項：(1) **回答神諭**（answer oracle）：要求根據給定的上下文回答問題，並引用相應的句子；(2) **相關性神諭**（relevance oracle）：要求列出上下文中與問題相關的任何資訊，並引用相應的句子；(3) **直接神諭**（straightforward oracle）：要求輸出回答給定問題的句子索引。我們發現，直接神諭的行為在不同的提示下有所不同，而使用回答神諭則使答案更具一致性。相關性神諭的動機是，上下文通常包含與查詢有遠距離相關的資訊，因此選擇相應的句子可能是合理的。比較列出的提示，我們觀察到相關性神諭的表現不如回答神諭，而直接神諭的表現與回答神諭相似或略低，因此我們在所有其他實驗中都使用了回答神諭。

**與重排序器的整合。** 在圖 2 中，我們比較了作為獨立模型訓練的 Provence 和與重排序器整合的統一模型，發現兩種策略都能產生相似的結果，儘管前者在 RAG 管線中依賴兩個獨立的推論步驟（重排序和剪枝）。

---

![](https://web-api.textin.com/ocr_image/external/dd1e362d5e1db7fb.jpg)

**圖 4：消融實驗結果。** 所有模型都是錨點模型的單一組件修改，錨點模型是基於 NQ 資料、使用回答神諭和 token 級別標記訓練的基礎大小模型。本圖的數值分數在附錄表 10 中重複，而基於匹配的指標結果則在附錄圖 11 中呈現。

---

## 5. 結論

在本研究中，我們提出了 Provence，一個用於問答的強健、可適應且高效的上下文剪枝器，它既可以作為一個與重排序功能整合的單一模型，也可以作為一個輕量級的獨立模型。與之前的抽取式方法相比，Provence 能夠針對給定的上下文動態偵測所需的剪枝率，並且可以即用（out-of-the-box）於多種問答領域。在大量的實驗中，我們證明了 Provence 能夠在效能幾無下降的情況下剪枝上下文，在某些情況下甚至因移除上下文雜訊而帶來效能提升。我們還展示了 Provence 在正確偵測上下文中任意位置的相關句子數量以及處理不同長度上下文方面的能力。最後，消融實驗凸顯了使用大型訓練資料和在銀標籤神諭中使用適當提示的重要性。

**限制。** 儘管 Provence 已證明可以在多種設定下即用，但它僅專注於問答應用，一次只處理一個段落，並且僅在英文資料上進行訓練。未來的工作可以考慮將其擴展到其他任務、多段落上下文，以及英語以外的其他語言。---
### A. 資料

**評估資料集。** 我們使用了以下資料集：

- **以維基百科為資料庫的資料集：**
  - **Natural Questions** (Kwiatkowski et al., 2019)：我們使用了 KILT 集合中分發的 2.8k 個問題測試集。
  - **HotpotQA** (Yang et al., 2018)：我們使用了 KILT 集合中分發的 5.6k 個問題測試集。
  - **PopQA** (Mallen et al., 2023b)：我們使用了由資料集作者分發的 14k 個問題測試集。

- **具有獨立資料庫的資料集：**
  - **BioASQ** (Nentidis et al., 2023)：我們使用了由 Hsia et al. (2024) 提供的資料集版本，包含 3.8k 個查詢。我們僅使用了「yes/no」、「factoid」和「list」類別的查詢。
  - **Syllabus QA** (Fernandez et al., 2024)：我們使用了由作者分發的 1.1k 個問題測試集。
  - **RGB** (Chen et al., 2024b)：我們使用了由作者分發的 200 個英文問題測試集。

所有資料集都為每個查詢提供了簡短的答案（關鍵字），我們使用這些答案來評估基於匹配的指標（如 Recall）和基於 LLM 的指標 (Rau et al., 2024a)¹。

**資料庫。** 為了訓練 **Provence**，我們使用了 MS MARCO 文件集合 (Craswell et al., 2021)。我們將每個文件分割成具有重疊的 $N$ 個句子區塊，其中 $N$ 是介於 1 到 10 之間的隨機數，以較高的機率產生較長的上下文，以訓練 Provence 處理不同長度的上下文。每個區塊前都加上頁面標題。最終的資料庫包含 34M 個段落。我們還以類似方式處理了維基百科資料庫，用於消融實驗。我們下載了 2024 年的維基百科轉儲，並使用 Pyserini (Lin et al., 2021)² 提供的腳本進行處理。我們還準備了這個維基百科資料庫的版本，其段落包含 $N$ 個句子並有 $N/2$ 個句子的重疊，用於測試 Provence 對不同上下文長度的強健性。

所有其他基於維基百科資料集的評估（包括主要評估）都是在以下維基百科資料庫上進行的：[https://huggingface.co/datasets/castorini/odqa-wiki-corpora](https://huggingface.co/datasets/castorini/odqa-wiki-corpora)。我們使用了一個版本，其段落包含 6 個句子並有 3 個句子的重疊，總共產生了 9M 個段落。

對於 Pubmed，我們使用了由 Hsia et al. (2024) 在 [https://huggingface.co/datasets/jenhsia/ragged](https://huggingface.co/datasets/jenhsia/ragged) 提供的資料集版本。它包含了從 Pubmed 摘要中提取的 58M 個段落。每個段落（區塊）前都加上了頁面標題。

對於 SyllabusQA，我們將每個大綱（由作者提供）分割成 100 個詞的段落（每個段落前都加上標題）。對於 RGB，檢索到的上下文由作者提供；我們為每個問題提供了 3 個相關和 2 個不相關的上下文（60% 的雜訊上下文）。

---
### B. 模型

表 5 列出了用於 **Provence** 實驗的所有主要模型及其對應的 HuggingFace 檢查點。

---
**表 5：實驗中使用的所有模型及其對應的 HuggingFace 檢查點列表。**

| 模型 | 檢查點 |
|---|---|
| SPLADE-v3 | `naver/splade-v3` |
| RetroMAE | `Shitao/RetroMAE_MSMARCO_distill` |
| DeBERTa-v3 | `microsoft/deberta-large` |
| DeBERTa-v3 (RR) | `naver/trecdl22-crossencoder-debertav3` |
| BGE-M3 | `BAAI/bge-reranker-v2-m3` |
| LLama-2-7B-chat | `meta-llama/Llama-2-7b-chat-hf` |
| LLaMA-3-8B-Instruct | `meta-llama/Meta-Llama-3-8B-Instruct` |
| Mistral-7B-instruct | `mistralai/Mistral-7B-Instruct-v0.2` |
| SOLAR-10.7B-Instruct-v1.0 | `upstage/SOLAR-10.7B-Instruct-v1.0` |

---
![](https://web-api.textin.com/ocr_image/external/e1d5b8ab8663098d.jpg)
![](https://web-api.textin.com/ocr_image/external/fc1a3c73ed3281bb.jpg)
**圖 5：由 LLaMA-3-8B-Instruct 標記的銀標籤上下文統計。**
（左）銀標籤上下文中句子數量的分佈。（右）選定句子在上下文中位置的分佈。

---
**表 6：用於 LLaMA-3-8B-Instruct 生成銀標籤的提示。回應中的句子引用使用正規表達式解析。**

| 問題：`{question}`<br>上下文：`[1] {sentence1} [2] {sentence2} [3] {sentence3} ...`<br>請**僅**使用上下文中提供的資訊來回答問題。如果沒有提供任何有用資訊，你**必須**輸出「沒有答案」。如果使用上下文的某些部分來回答，你**必須引用**所有對應的句子。請使用方括號 `[ ]` 來表示事實來自上下文中的哪個句子，例如 `[0]` 表示來自第 0 個句子的事實。你只應回答給定的問題，不應提供任何額外資訊。 |
| --- |

---
**表 7：13 個開放 BEIR 資料集的 nDCG@10。**

| 語料庫 | DeBERTav3 | Provence |
|---|---|---|
| TREC-COVID | 88.3 | 88.3 |
| NFCorpus | 37.5 | 37.8 |
| NQ | 66.7 | 66.5 |
| HotpotQA | 74.5 | 74.9 |
| FiQA-2018 | 47.8 | 47.6 |
| ArguAna | 29.8 | 33.2 |
| Touché-2020 | 33.5 | 33.4 |
| Quora | 84.8 | 85.4 |
| DBPedia | 48.9 | 49.2 |
| SCIDOCS | 19.2 | 19.6 |
| FEVER | 86.6 | 87.9 |
| Climate-FEVER | 27.4 | 28.1 |
| 平均值 | 55.4 | 55.9 |

---
![](https://web-api.textin.com/ocr_image/external/0d986fb733360b30.jpg)
**圖 6：可視化單個 token 被選入最終上下文的機率示例。**

---
![](https://web-api.textin.com/ocr_image/external/605fae6503139ca5.jpg)
**圖 7：各種問答領域的主要結果，比較 Provence 和基線模型，指標：Recall。**
生成器：LLama-2-7B，檢索器：SPLADE-v3，重排序器：DeBERTa-v3（或統一設定下的 Provence）。圖標題表示「資料集名稱（資料庫類型）」。x 軸表示使用 Recall 評估的問答效能；y 軸表示上下文壓縮率。兩個指標都是越高越好：最佳模型應最接近右上角。

---
![](https://web-api.textin.com/ocr_image/external/63d26c03cd9b6d2c.jpg)
**圖 8：在不同 RAG 設定下（檢索、重排序、生成器）測試 Provence。**

---
![](https://web-api.textin.com/ocr_image/external/90395438e5813908.jpg)
**圖 9：將 Provence 與部分基線模型進行比較，檢索器：RetroMAE (Shitao et al., 2022)，重排序器：BGE-M3 (Chen et al., 2024a)，生成器：LLama-2-7B-chat。**

---
![](https://web-api.textin.com/ocr_image/external/622dfc5aba4c6f99.jpg)
**圖 10：使用提供給生成器的不同 $top-k$ 文件測試 Provence。**
設定與圖 2 相同。

---
**表 8：對應於圖 2 的數值分數 - NQ, Hotpot QA, Tydi QA, Pop QA。**

| | NQ | NQ | HotPot QA | HotPot QA | Tydi QA | Tydi QA | PopQA | PopQA |
|---|---|---|---|---|---|---|---|---|
| | LLM Eval | 壓縮率 % | LLM Eval | 壓縮率 % | LLM Eval | 壓縮率 % | LLM Eval | 壓縮率 % |
| 完整上下文 | 71.8 | 0.0 | 57.0 | 0.0 | 73.9 | 0.0 | 57.8 | 0.0 |
| Provence | 72.4 | 62.2 | 56.7 | 66.4 | 70.5 | 63.0 | 59.3 | 68.6 |
| （含重排序） | 72.6 | 76.0 | 56.0 | 82.4 | 73.6 | 76.2 | 59.5 | 75.8 |
| Provence | 72.3 | 64.1 | 56.6 | 69.5 | 70.9 | 65.8 | 59.0 | 69.9 |
| （獨立） | 70.6 | 77.3 | 54.8 | 84.1 | 70.2 | 78.1 | 58.8 | 76.1 |
| LLMLingua2 | 59.5 | 74.0 | 47.1 | 74.4 | 57.7 | 73.9 | 42.9 | 75.0 |
| LLMLingua2 | 67.5 | 45.4 | 52.9 | 45.8 | 67.3 | 45.0 | 52.5 | 46.3 |
| LLMLingua2 | 70.3 | 25.0 | 55.0 | 24.9 | 70.0 | 24.8 | 55.2 | 25.1 |
| LongLLMLingua | 61.3 | 69.1 | 52.6 | 68.5 | 56.6 | 69.5 | 49.5 | 65.5 |
| LongLLMLingua | 68.5 | 47.9 | 55.6 | 46.5 | 65.5 | 47.8 | 54.5 | 43.6 |
| LongLLMLingua | 71.3 | 28.7 | 56.9 | 26.8 | 69.1 | 28.8 | 57.6 | 23.9 |
| RECOMP | 70.6 | 43.6 | 55.5 | 40.9 | 68.6 | 46.4 | 56.9 | 41.1 |
| (ext) | 68.2 | 59.8 | 53.4 | 57.5 | 67.0 | 63.0 | 55.7 | 57.4 |
| RECOMP | 66.2 | 77.1 | 50.1 | 75.7 | 64.5 | 79.7 | 52.3 | 74.9 |
| (ext+thr) | 69.0 | 59.5 | 50.9 | 62.9 | 70.9 | 52.4 | 54.8 | 56.0 |
| RECOMP | 72.9 | 11.8 | 56.4 | 14.4 | 72.3 | 6.9 | 58.5 | 12.4 |
| (abs) | 66.9 | 94.5 | 53.1 | 94.4 | 66.4 | 95.2 | 54.4 | 92.8 |
| DSLR | 71.7 | 44.9 | 52.9 | 75.7 | 72.7 | 45.8 | 58.6 | 48.1 |
| DSLR | 70.5 | 54.9 | 50.7 | 83.4 | 69.8 | 55.6 | 58.7 | 58.1 |
| DSLR | 70.4 | 61.4 | 49.3 | 87.0 | 70.7 | 62.0 | 58.8 | 63.7 |
| DSLR | 67.7 | 72.0 | 45.2 | 91.7 | 67.5 | 72.9 | 58.5 | 71.9 |
| DSLR | 67.6 | 77.7 | 43.2 | 93.4 | 67.5 | 78.1 | 57.9 | 76.0 |
| Dense-X | 62.7 | 69.0 | 49.6 | 67.7 | 66.4 | 71.5 | 52.0 | 68.5 |

---
**表 9：對應於圖 2 的數值分數 - Syllabus QA, BioASQ, RGB。**

| | Syllabus QA | Syllabus QA | BioASQ | BioASQ | RGB | RGB |
|---|---|---|---|---|---|---|
| | LLM Eval | 壓縮率 % | LLM Eval | 壓縮率 % | LLM Eval | 壓縮率 % |
| 完整上下文 | 52.9 | 0.0 | 80.7 | 0.0 | 93.5 | 0.0 |
| Provence | 49.8 | 60.6 | 80.6 | 49.0 | 94.4 | 60.5 |
| （含重排序） | 51.0 | 76.5 | 80.3 | 67.4 | 96.3 | 69.3 |
| Provence | 50.7 | 64.1 | 80.6 | 51.3 | 95.8 | 61.6 |
| （獨立） | 47.8 | 76.6 | 80.1 | 68.9 | 96.3 | 69.4 |
| LLMLingua2 | 37.4 | 73.4 | 72.6 | 73.6 | 78.6 | 74.3 |
| LLMLingua2 | 43.4 | 45.4 | 77.7 | 45.2 | 93.5 | 46.1 |
| LLMLingua2 | 49.8 | 26.6 | 78.7 | 24.8 | 95.8 | 26.3 |
| LongLLMLingua | 42.3 | 71.3 | 72.2 | 72.9 | 71.6 | 73.9 |
| LongLLMLingua | 45.1 | 48.5 | 77.3 | 50.4 | 83.3 | 51.6 |
| LongLLMLingua | 50.9 | 29.2 | 78.7 | 31.3 | 92.1 | 32.1 |
| RECOMP | 44.6 | 51.5 | 78.7 | 42.2 | 97.7 | 42.0 |
| (ext) | 42.7 | 61.4 | 78.4 | 54.8 | 94.9 | 52.1 |
| RECOMP | 39.1 | 71.1 | 76.3 | 69.7 | 94.4 | 63.2 |
| (ext+thr) | 45.5 | 35.7 | 76.6 | 51.2 | 92.1 | 51.4 |
| RECOMP | 52.8 | 7.7 | 80.2 | 6.5 | 97.7 | 13.4 |
| (abs) | 38.1 | 98.9 | 68.2 | 96.1 | 90.7 | 95.7 |
| DSLR | 49.6 | 33.2 | 80.1 | 29.9 | 97.2 | 41.6 |
| DSLR | 49.1 | 46.4 | 79.6 | 40.1 | 97.7 | 46.9 |
| DSLR | 47.2 | 55.4 | 79.2 | 47.3 | 96.3 | 49.7 |
| DSLR | 44.2 | 70.6 | 77.6 | 60.2 | 97.2 | 54.1 |
| DSLR | 40.7 | 78.2 | 75.4 | 68.0 | 95.8 | 56.9 |

---
![](https://web-api.textin.com/ocr_image/external/e3c6c4b8f76279dd.jpg)
**圖 11：使用 Recall（基於匹配的指標）進行的消融實驗結果。**

---
**表 10：對應於圖 4 的數值分數。**

| | 門檻 | NQ LLM Eval | NQ 壓縮率 % | HotPot QA LLM Eval | HotPot QA 壓縮率 % | Syllabus QA LLM Eval | Syllabus QA 壓縮率 % |
|---|---|---|---|---|---|---|---|
| Anchor | 0.01 | 72.2 | 46.9 | 56.0 | 46.7 | 51.1 | 33.1 |
| Anchor | 0.1 | 71.6 | 66.6 | 55.6 | 70.8 | 51.1 | 65.5 |
| Anchor | 0.5 | 70.6 | 79.6 | 52.9 | 85.8 | 44.0 | 78.7 |
| Deberta-large | 0.01 | 72.5 | 59.4 | 56.7 | 63.7 | 49.9 | 50.7 |
| Deberta-large | 0.1 | 72.1 | 69.2 | 55.9 | 75.6 | 49.6 | 68.3 |
| Deberta-large | 0.5 | 70.9 | 78.0 | 53.2 | 84.7 | 46.1 | 77.0 |
| MS-Marco | 0.01 | 72.9 | 27.1 | 57.2 | 25.9 | 52.5 | 11.6 |
| (downsample) | 0.1 | 72.8 | 57.5 | 56.2 | 61.6 | 50.5 | 49.3 |
| MS-Marco | 0.5 | 68.7 | 79.4 | 52.3 | 84.5 | 43.0 | 78.4 |
| (downsample) | 0.1 | 72.3 | 64.1 | 56.6 | 69.5 | 50.7 | 64.1 |
| MS-Marco | 0.5 | 70.6 | 77.3 | 54.8 | 84.1 | 47.8 | 76.6 |
| (full) | 0.01 | 73.1 | 31.1 | 56.8 | 32.8 | 52.9 | 25.6 |
| straight.oracle | 0.1 | 72.5 | 51.4 | 55.9 | 56.8 | 51.7 | 54.7 |
| straight.oracle | 0.5 | 72.0 | 70.2 | 54.8 | 76.8 | 45.5 | 75.3 |
| relevant oracle | 0.01 | 72.8 | 16.1 | 57.4 | 22.6 | 49.4 | 11.1 |
| relevant oracle | 0.1 | 72.6 | 38.8 | 55.7 | 55.1 | 49.6 | 41.4 |
| relevant oracle | 0.5 | 71.3 | 66.9 | 51.6 | 83.9 | 46.8 | 73.3 |
| sent-level | 0.01 | 73.0 | 51.2 | 56.2 | 54.5 | 51.5 | 47.2 |
| labeling | 0.1 | 71.0 | 66.4 | 55.0 | 72.5 | 48.3 | 69.2 |
| sent-level | 0.5 | 71.2 | 78.2 | 53.0 | 85.3 | 40.4 | 78.8 |
| labeling | 0.01 | 71.8 | 0.0 | 57.0 | 0.0 | 52.9 | 0.0 |
| full context | 0.01 | 71.8 | 0.0 | 57.0 | 0.0 | 52.9 | 0.0 |

---
**表 11：使用各種方法進行上下文剪枝的範例。**
**Provence** 選取了一個關於牧羊人派（Shepherd’s pie）的句子，並移除了關於其他類似菜餚的句子，這是 **RECOMP (ext)** 無法做到的。**RECOMP (abs)** 正確地生成了摘要；**LongLLMLingua** 移除了與牧羊人派相關的部分，而 **LLMLingua2** 則均勻地移除了沒有資訊的 token。

| 問題 | 牧羊人派的底部是什麼？ |
|---|---|
| **原始上下文** | **牧羊人派。** 歷史。在早期的烹飪書中，這道菜是用來利用任何種類的剩餘烤肉，派盤的側面和底部都鋪上了馬鈴薯泥，頂部也有馬鈴薯泥外殼。變體和類似菜餚。其他以馬鈴薯為頂部的派包括：現代的「坎伯蘭派」（Cumberland pie）是一種使用了牛肉或羊肉，並在頂部鋪上麵包屑和起司的版本。在中世紀以及現代的坎布里亞郡，其酥皮外殼的餡料是肉、水果和香料。在魁北克，一種鄉村派的變體被稱為「Pâté chinois」。它以底層的絞牛肉、中間層的罐裝玉米和頂層的馬鈴薯泥製成。而「牧羊女派」（shepherdess pie）是一種不含肉的素食版本，或是不含肉和乳製品的純素版本。在荷蘭，一種非常相似的菜餚被稱為「哲學家燉菜」（philosopher’s stew），通常會添加豆類、蘋果、梅子或蘋果醬等食材。在巴西，有一道菜稱為，指的是木薯泥藏著一層曬乾的肉。 |
| **Provence** | **牧羊人派。** 在早期的烹飪書中，這道菜是用來利用任何種類的剩餘烤肉，派盤的側面和底部都鋪上了馬鈴薯泥，頂部也有馬鈴薯泥外殼。 |
| **RECOMP (ext, top-3)** | 在早期的烹飪書中，這道菜是用來利用任何種類的剩餘烤肉，派盤的側面和底部都鋪上了馬鈴薯泥，頂部也有馬鈴薯泥外殼。而「牧羊女派」是一種不含肉的素食版本，或是不含肉和乳製品的純素版本。在荷蘭，一種非常相似的菜餚被稱為「哲學家燉菜」，通常會添加豆類、蘋果、梅子或蘋果醬等食材。 |
| **RECOMP (abs)** | 牧羊人派是一道用剩餘的烤肉和底部的馬鈴薯泥製成的菜餚，其變體包括使用牛肉或羊肉，並在頂部鋪上麵包屑和起司的版本。這道菜的歷史是作為利用剩餘烤肉的方法，並在頂部配料上有多種變體。 |
| **LLMLingua2** (comp.50%) | 牧羊人派 歷史 早期烹飪書 菜餚 剩餘烤肉 鋪上 馬鈴薯泥 馬鈴薯泥 外殼 頂部 變體 類似 菜餚 以馬鈴薯為頂部 派 現代 「坎伯蘭派」 牛肉 羊肉 麵包屑 起司 中世紀 現代 坎布里亞郡 酥皮 外殼 餡料 肉 水果 香料 魁北克 變體 鄉村派 「Pâté 絞牛肉 底部 罐裝玉米 中間 馬鈴薯泥 頂部 「牧羊女派」 素食 不含肉 純素 版本 不含肉 乳製品 荷蘭 類似 菜餚 「哲學家燉菜」 添加 食材 豆類 蘋果 梅子 蘋果醬 巴西 菜餚 木薯泥 隱藏 曬乾肉 |
| **LongLLMLingua** (comp. 50%) | 牧羊人派。其他以馬鈴薯為頂部的派包括：現代的「坎伯蘭派」是一種使用了牛肉或羊肉，並在頂部鋪上麵包屑和起司的版本。在中世紀以及現代的坎布里亞郡，其酥皮外殼的餡料是肉、水果和香料。在魁北克，一種鄉村派的變體被稱為「Pâté chinois」。它以底層的絞牛肉、中間層的罐裝玉米和頂層的馬鈴薯泥製成。而「牧羊女派」是一種不含肉的素食版本，或是不含肉和乳製品的純素版本。在荷蘭，一種非常相似的菜餚被稱為「哲學家燉菜」，通常會添加豆類、蘋果、梅子或蘋果醬等食材。在巴西，有一道菜稱為，指的是木薯泥藏著一層曬乾的肉。 |

---
**表 12：使用各種方法進行上下文剪枝的範例。**
**Provence** 正確地偵測到整個段落都與查詢相關，這點與 **LongLLMLingua** 相同，而 **RECOMP (ext)** 則無法做到這樣的決策。

| 問題 | 水果的甜味來自哪裡 |
|---|---|
| **原始上下文** | **甜味。** 許多植物物種產生的糖苷，其甜度在遠低於糖的濃度下就很高。最著名的例子是甘草根中的甜味成分甘草甜素，其甜度約是蔗糖的 30 倍。另一個具有商業重要性的例子是甜菊苷，來自南美灌木「Stevia rebaudiana」。它的甜度約是蔗糖的 250 倍。另一類強效的天然甜味劑是甜蛋白，例如在西非卡特姆費果中發現的索馬甜。雞蛋中的一種抗生素蛋白溶菌酶也具有甜味。 |
| **Provence** | **甜味。** 許多植物物種產生的糖苷，其甜度在遠低於糖的濃度下就很高。最著名的例子是甘草根中的甜味成分甘草甜素，其甜度約是蔗糖的 30 倍。另一個具有商業重要性的例子是甜菊苷，來自南美灌木「Stevia rebaudiana」。它的甜度約是蔗糖的 250 倍。另一類強效的天然甜味劑是甜蛋白，例如在西非卡特姆費果中發現的索馬甜。雞蛋中的一種抗生素蛋白溶菌酶也具有甜味。 |
| **RECOMP (ext, top-3)** | 它的甜度約是蔗糖的 250 倍。另一個具有商業重要性的例子是甜菊苷，來自南美灌木「Stevia rebaudiana」。許多植物物種產生的糖苷，其甜度在遠低於糖的濃度下就很高。 |
| **RECOMP (abs)** | [空上下文] |
| **LLMLingua2** (comp.50%) | 甜味 植物物種 產生 糖苷 甜度 低於 糖 甘草甜素 甜味 甘草根 30 倍 甜 蔗糖 甜菊苷 南美 灌木 「Stevia 250 倍 甜 蔗糖 甜味劑 甜蛋白 索馬甜 西非 卡特姆費果 溶菌酶 抗生素 蛋白 雞蛋 甜味 |
| **LongLLMLingua** (comp. 50%) | 甜味。許多植物物種產生的糖苷，其甜度在遠低於糖的濃度下就很高。最著名的例子是甘草根中的甜味成分甘草甜素，其甜度約是蔗糖的 30 倍。另一個具有商業重要性的例子是甜菊苷，來自南美灌木「Stevia rebaudiana」。它的甜度約是蔗糖的 250 倍。另一類強效的天然甜味劑是甜蛋白，例如在西非卡特姆費果中發現的索馬甜。雞蛋中的一種抗生素蛋白溶菌酶也具有甜味。 |

---
**表 13：使用各種方法進行上下文剪枝的範例。**
**Provence** 選取了最相關的一個句子，這也是 **RECOMP (ext)** 排名第一的句子。**RECOMP (abs)** 認為沒有資訊與查詢相關，而 **LongLLMLingua** 則保留了整個輸入，但移除了一些標點符號。**LLMLingua2** 移除了過多的 token，使得文本難以理解。

| 問題 | 倫敦塔最初是做什麼用的？ |
|---|---|
| **原始上下文** | **倫敦塔。** 在 16 世紀，倫敦塔獲得了作為一個陰森、令人畏懼的監獄的持久聲譽。情況並非總是如此。作為一座皇家城堡，它被君主用來監禁各種原因的人，然而這些通常是高階級的個人，且監禁時間很短，而不是普通公民，因為其他地方有足夠多的監獄可以關押他們。與倫敦塔的流行印象相反，囚犯可以透過向倫敦塔中尉購買更好的食物或掛毯等便利設施，使他們的生活變得更輕鬆。由於關押囚犯最初只是倫敦塔的附帶角色——這對任何城堡來說都是如此——直到 1687 年，才建造了一個磚砌棚屋，一個「士兵監獄」，位於白塔的西北側，作為專門為囚犯設計的住宿。倫敦塔關於折磨和監禁的聲譽主要來自 16 世紀的宗教宣傳家和 19 世紀的浪漫主義者。 |
| **Provence** | **倫敦塔。** 作為一座皇家城堡，它被君主用來監禁各種原因的人，然而這些通常是高階級的個人，且監禁時間很短，而不是普通公民，因為其他地方有足夠多的監獄可以關押他們。 |
| **RECOMP (ext, top-3)** | 作為一座皇家城堡，它被君主用來監禁各種原因的人，然而這些通常是高階級的個人，且監禁時間很短，而不是普通公民，因為其他地方有足夠多的監獄可以關押他們。情況並非總是如此。倫敦塔關於折磨和監禁的聲譽主要來自 16 世紀的宗教宣傳家和 19 世紀的浪漫主義者。 |
| **RECOMP (abs)** | [空上下文] |
| **LLMLingua2** (comp.25%) | 倫敦塔 16 世紀 陰森 監獄 皇家城堡 君主 高階級 普通公民 囚犯 便利設施 食物 中尉 倫敦塔 專門為囚犯 設計住宿 直到 1687 「士兵監獄」 西北側 白塔 聲譽 折磨 監禁 16 世紀 宣傳家 19 世紀 浪漫主義者 |
| **LongLLMLingua** (comp. 25%) | 倫敦塔 在 16 世紀，獲得了作為一個陰森、令人畏懼的監獄的持久聲譽。情況並非總是如此 作為一座皇家城堡，它被用來監禁各種原因的人，然而這些通常是高階級的個人，且監禁時間很短 而不是普通公民，因為其他地方有足夠多的監獄可以關押他們。與 倫敦塔的流行印象相反，囚犯可以透過購買更好的食物或掛毯等便利設施，使他們的生活變得更輕鬆。關押囚犯最初只是倫敦塔的附帶角色 — 這對任何城堡來說都是如此 — 直到 167 年，一個「士兵監獄」才建成，位於 白塔的西北側。倫敦塔關於折磨和監禁的聲譽主要來自 16 世紀的宗教宣傳家和 19 世紀的浪漫主義者。 |