# Provence：高效且魯棒的上下文裁剪器

## 簡介

Provence 是一個專為大型語言模型（LLMs）的問答（Question Answering）應用所設計的高效**上下文裁剪器**。它旨在解決 RAG（檢索增強生成）範式中，因處理**過長上下文**而導致的計算開銷與**不相關資訊傳播**問題。Provence 透過移除檢索到的不相關內容，能在 LLM 生成前有效縮短上下文長度，從而加快生成速度並提升準確性。

### 什麼是 RAG 範式？

**檢索增強生成（RAG）**是一種廣泛使用的技術，用於提高 LLMs 的**事實性、歸因和適應性**。它通過為使用者查詢檢索相關上下文，來避免模型生成不真實的資訊，並提供生成答案的參考依據。RAG 的一個主要優勢是其**即插即用（plug-and-play）**的架構，開發者可以根據需求靈活選擇不同的組件。

## Provence 的核心功能與技術細節

### 核心要素

Provence 的設計基於以下幾個關鍵要素：

- **將上下文裁剪表述為序列標註**：模型為上下文中的每個單詞或句子輸出一個二進制標籤，用來判斷該部分是否應被保留。
- **統一裁剪與重新排序**：Provence 能夠在單次前向傳播中同時執行這兩項任務，顯著降低了額外的計算成本。
- **多樣化資料訓練**：模型在多個領域的資料上進行訓練，使其能夠**開箱即用**於各種不同的應用情境。

### 技術細節與分析

#### 訓練資料與標籤生成

- **訓練資料集**：使用 **MS MARCO** 文件排序集合（370k 查詢）和 **Natural Questions**（87k 查詢）的訓練集。MS MARCO 的多樣性有助於模型的魯棒性。
- **銀標籤生成**：訓練標籤是透過提示 **Llama-3-8B-Instruct** 模型來生成的，該模型會根據提供的上下文回答問題並引用所有相關句子。

#### 模型架構與訓練

- **基於 DeBERTa**：Provence 透過微調 **DeBERTa** 模型來實現。
- **統一模型的訓練**：模型從一個預訓練的**跨編碼器（cross-encoder）**開始，並添加了一個排序「正規化器」，以保留其初始的重新排序能力。模型以**每標記二進制分類器**的形式進行訓練。

#### 推理過程

1. Provence 接收查詢與檢索到的段落。
2. 輸出每個標記被保留在最終上下文中的**概率**。
3. 透過設定一個**閾值 T** 來二值化這些概率，從而控制上下文的壓縮率。
4. 應用「**句子四捨五入**」程序：若一個句子中被保留的標記比例超過 0.5，則選取整個句子，以避免部分句子被選中。

> **提示**：Provence 雖然輸出標記級別的預測，但由於訓練時使用句子級別的目標，這些概率自然會在句子級別上聚類。

#### 效率與性能

- Provence 在不同領域和設定下進行裁剪，**性能下降可忽略不計**，且幾乎不增加標準 RAG 管線的成本。
- 與其他方法相比，Provence 尤其在統一模型中表現出更高的效率，裁剪功能幾乎是「**免費**」獲得的。

#### 魯棒性分析

- **相關資訊位置**：透過「大海撈針」實驗，Provence 即使在訓練資料較少的位置，也能正確選取相關句子。
- **相關句子數量**：Provence 能夠動態適應上下文中相關句子的數量（從零到全部）。
- **上下文粒度**：在不同長度的上下文（例如 2、6、10 個句子或 100 個單詞）下，均表現出高性能。
- **重新排序**：聯合訓練程序確保 Provence 在學習裁剪的同時，能有效保留其初始的重新排序能力。

---

### 消融實驗與限制

- **消融實驗結果**：研究顯示，模型大小、訓練資料集和標籤生成策略對 Provence 的性能有顯著影響。
- **目前限制**：
  - 專注於**問答**應用。
  - 一次僅處理**單個段落**。
  - 目前僅針對**英語數據**進行訓練。

未來的研究可能會將其擴展到更多任務、多段落上下文和更多語言。